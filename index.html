<!DOCTYPE html>
<html lang="en">

<head>
  <title>Shuyue (Bruce) Jia</title>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Shuyue Jia, è´¾èˆ’è¶Š, Bruce Jia, Jia Shuyue, Bruce Shuyue Jia, Bruce S. Jia, Shuyue, shuyue, shuyue jia, jia shuyue, jiashuyue, shuyuejia, shuyuej, èˆ’è¶Š, èˆ’è¶Šè´¾, è´¾ èˆ’è¶Š, èˆ’è¶Š è´¾, shuyu jia, jia shuyu, èˆ’, è¶Š, è´¾, è´¾èˆ’, è´¾è¶Š, è´¾ èˆ’ è¶Š, SHUYUE JIA, SHUYUE, JIA SHUYUE, JIASHUYUE, SHU, YUE, JIA, JIASHU, JIA SHU, JIA SHU YUE, Bruce Jia, Bruce, Bruce Shuyue Jia, Bruce Shuyue, Bruce Shuyue JIA, SHUYUE JIA, SHUYUE, JIA SHUYUE, JIASHUYUE, SHU, YUE, JIA, JIASHU, JIA SHU, JIA SHU YUE">
    <meta name="author" content="Shuyue Jia">
    <meta name="description" content="This is Shuyue Jia's Homepage! Reference: Shuyue Jia, è´¾èˆ’è¶Š, Bruce Jia, Jia Shuyue, Bruce Shuyue Jia, Bruce S. Jia, Shuyue, shuyue, shuyue jia, jia shuyue, jiashuyue, shuyuejia, shuyuej, èˆ’è¶Š, èˆ’è¶Šè´¾, è´¾ èˆ’è¶Š, èˆ’è¶Š è´¾, shuyu jia, jia shuyu, èˆ’, è¶Š, è´¾, è´¾èˆ’, è´¾è¶Š, è´¾ èˆ’ è¶Š, SHUYUE JIA, SHUYUE, JIA SHUYUE, JIASHUYUE, SHU, YUE, JIA, JIASHU, JIA SHU, JIA SHU YUE, Bruce Jia, Bruce, Bruce Shuyue Jia, Bruce Shuyue, Bruce Shuyue JIA, SHUYUE JIA, SHUYUE, JIA SHUYUE, JIASHUYUE, SHU, YUE, JIA, JIASHU, JIA SHU, JIA SHU YUE">
    <meta name="google-site-verification" content="UZ7grdeibwJYn35EsW-Yp0Ky7_vuMG45bHI1kki15H0"/>
    <meta name="baidu-site-verification" content="WJpwHTPWUq"/>
    
    <style>
      div{
        line-height:28px;
      }
      span.highlight {
        background-color: #ffffd0;
      }
    </style>
    
    <link rel="shortcut icon" href="bu.ico">
    <link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700" rel="stylesheet" type="text/css">
    <link rel="alternate" type="application/rss+xml" title="Shuyue Jia RSS" href="/feed.xml" />
    <link rel="stylesheet" href="/css/main.css">
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="stylesheet" href="css/css/academicons.min.css"/>
    <link href="css/main.css" rel="stylesheet" type="text/css">
</head>

<style type="text/css">
    p{
      text-align:justify;
    }
    
    p i{
      display:inline-block;
      width:100%;
    }
    
    ul{
      text-align:justify;
    }
    
    ul i{
      display:inline-block;
      width:100%;
    }
</style>

<style>
  .arrow-list {
    list-style-type: none; /* Removes default bullets */
    padding-left: 0;
    margin: 0; /* Remove extra margins around the list */
  }
  
  .arrow-list li {
    font-size: 24px; /* Set font size */
    line-height: 1.0; /* Shorter line spacing */
  }
  
  .arrow-list li::before {
    content: 'â†’'; /* Unicode rightward arrow */
    margin-right: 8px; /* Add space between arrow and text */
  }
</style>

<body>
<div class="wrapper">
  <div class="navbar container">
    <a id="author-name" class="alignable pull-left" style="text-decoration:none; display: flex; align-items: center; gap: 0.05em;">
      Shuyue Jia (Bruce)
    </a>
  </div>
<div style="clear:both"></div>

<hr>

<div class="container content">

<h2 id="about-me">Bio</h2>
<div class="profile-container">
  <img class="profile-picture" src="./imgs/Profile/Big-Buddha-Shuyue-Profile.jpg"/>
  <p class="image-description">ğ’¯ğ’¾ğ’¶ğ“ƒ ğ’¯ğ’¶ğ“ƒ ğµğ“Šğ’¹ğ’¹ğ’½ğ’¶, ğ»ğ‘œğ“ƒğ‘” ğ’¦ğ‘œğ“ƒğ‘”</p>
</div>

<p>
  I am a Ph.D. student in Computer Engineering at <a href="https://www.bu.edu">Boston University</a>, under the supervision of <a href="https://vkola-lab.github.io/team" style="color:black;">Prof. Vijaya Kolachalama</a>. Prior to BU, I received my M.Phil. degree (2023) in Computer Science from <a href="https://www.cityu.edu.hk">City University of Hong Kong</a>, supervised by <a href="https://www.cs.cityu.edu.hk/~shiqwang" style="color:black;">Prof. Shiqi Wang</a>, and B.Eng. degree (2020) in Intelligence Science and Technology from <a href="https://www.neepu.edu.cn">Northeast Electric Power University</a>, supervised by <a href="https://ieeexplore.ieee.org/author/37290052900" style="color:black;">Prof. Yimin Hou</a> and <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/jinglei-lv.html" style="color:black;">Prof. Jinglei Lv</a>. In 2017, I attended a summer school at <a href="https://uci.edu">University of California, Irvine</a>. I am a member of a research lab that emphasizes a ğ˜ğ—²ğ—®ğ—º-ğ—³ğ—¶ğ—¿ğ˜€ğ˜ culture and values ğ—¾ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜†, ğ—¶ğ—»ğ˜ğ—²ğ—´ğ—¿ğ—¶ğ˜ğ˜†, ğ—¹ğ—²ğ—®ğ—±ğ—²ğ—¿ğ˜€ğ—µğ—¶ğ—½, and ğ˜€ğ—²ğ—¿ğ˜ƒğ—¶ğ—°ğ—². My goal is to translate cutting-edge research into impactful clinical applications.
  
  <br>
  <br>
  
  Iâ€™m also a ğ—ªğ—²ğ—±ğ—±ğ—¶ğ—»ğ—´, ğ—£ğ—¼ğ—¿ğ˜ğ—¿ğ—®ğ—¶ğ˜, ğ—®ğ—»ğ—± ğ—˜ğ˜ƒğ—²ğ—»ğ˜ ğ—£ğ—µğ—¼ğ˜ğ—¼ğ—´ğ—¿ğ—®ğ—½ğ—µğ—²ğ—¿ based in Boston, MA. If youâ€™d like to book a session, Iâ€™d love to hear from youâ€”just send me an email!
  <a href="https://brucejia.myportfolio.com/">Check out my collections here!</a>

  <br>
  <br>
  
  ğ—ğ—²ğ˜† ğ—¤ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€:<br>
  <ul class="arrow-list">
    <li>What impact can a TEAM of top multidisciplinary scientists have?</li>
    <li>What kinds of VALUE can we researchers bring to society?</li>
    <li>What is the most surprising FINDING in your research?</li>
  </ul>

  <br>
  
  ğ—¥ğ—²ğ˜€ğ—²ğ—®ğ—¿ğ—°ğ—µ ğ—œğ—»ğ˜ğ—²ğ—¿ğ—²ğ˜€ğ˜ğ˜€:<br>
  My Ph.D. research focuses on <b>AI Grounding</b> [<a href="https://www.medrxiv.org/content/10.1101/2024.07.11.24310304v1">Continual Pre-training</a>, <a href="https://www.medrxiv.org/content/10.1101/2024.07.11.24310304v2">Retrieval-augmented Generation (RAG)</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1386505626000791">Agentic RAG System</a>] as a pathway toward <b>Artificial General Intelligence (AGI) for Medicine</b>, with an emphasis on developing safe, reliable, and extensible foundations. Specifically, my work focuses on the following dimensions:
  <br>
  <br>
  
  <li>ğ— ğ—²ğ—®ğ˜€ğ˜‚ğ—¿ğ—²ğ—ºğ—²ğ—»ğ˜
    <ul class="arrow-list">
      <li>Quality Estimation of Natural Language Generation (NLG)</li>
      <li>Quality Assessment of Natural Image [<a href="https://ieeexplore.ieee.org/abstract/document/9950035">1</a>, <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ell2.12698">2</a>]</li>
    </ul>
  </li>

  <li>ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¶ğ—»ğ—´
    <ul class="arrow-list">
      <li>Multimodal Foundation Model (FM) and Large Vision-language Model (LVLM)</li>
      <li>AI Workflow, Agentic System, and Multi-agent System (MAS) [<a href="https://www.sciencedirect.com/science/article/pii/S1386505626000791">1</a>, <a href="https://aclanthology.org/2025.emnlp-main.893">2</a>]</li>
      <li>Diffusion Model [<a href="https://ieeexplore.ieee.org/document/10566053">1</a>]</li>
      <li>Graph CNN (GCN) [<a href="https://ieeexplore.ieee.org/document/9889159">1</a>]</li>
      <li>Attention-based BiLSTM-GCN [<a href="https://www.frontiersin.org/journals/bioengineering-and-biotechnology/articles/10.3389/fbioe.2021.706229/full">1</a>]</li>
      <li>CNN [<a href="https://iopscience.iop.org/article/10.1088/1741-2552/ab4af6">1</a>]</li>
    </ul>
  </li>

  <li>ğ—¢ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—®ğ˜ğ—¶ğ—¼ğ—»
    <ul class="arrow-list">
      <li>Continual Pre-training of LLM [<a href="https://www.medrxiv.org/content/10.1101/2024.07.11.24310304v1">1</a>, <a href="https://www.nature.com/articles/s44385-025-00022-0">2</a>]</li>
      <li>Retrieval-augmented Generation (RAG) [<a href="https://www.nature.com/articles/s44385-025-00022-0">1</a>]</li>
    </ul>
  </li>

  <li>ğ—§ğ—¼ğ—¼ğ—¹ğ—¸ğ—¶ğ˜ & ğ—Ÿğ—¶ğ—¯ğ—¿ğ—®ğ—¿ğ˜† (ğ—¢ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜)
    <ul class="arrow-list">
      <li><a href="https://github.com/vkola-lab/PodGPT">PodGPT</a>: An audio-augmented LLM for research and education
        <iframe
          src="https://img.shields.io/github/stars/vkola-lab/PodGPT?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe>
        <iframe
          src="https://img.shields.io/github/forks/vkola-lab/PodGPT?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe> 
      </li>
      <li><a href="https://github.com/vkola-lab/PodGPT/tree/main/rag_pipeline">PodRAG</a>: PodGPT with a retrieval, reranking, and generation pipeline
        <iframe
          src="https://img.shields.io/github/stars/vkola-lab/PodGPT?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe>
        <iframe
          src="https://img.shields.io/github/forks/vkola-lab/PodGPT?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe>
      </li>
      <li><a href="https://github.com/SuperBruceJia/EEG-DL">EEG-DL</a>: A deep learning library for EEG signals classification
        <iframe
          src="https://img.shields.io/github/stars/SuperBruceJia/EEG-DL?style=social" 
          frameborder="0" scrolling="0" width="90px" height="20px">
        </iframe>
        <iframe
          src="https://img.shields.io/github/forks/SuperBruceJia/EEG-DL?style=social" 
          frameborder="0" scrolling="0" width="100px" height="20px">
        </iframe>
      </li>
      <li><a href="https://github.com/SuperBruceJia/promptcraft">Prompt Perturbation</a>: A perturbation toolkit for prompt augmentation
        <iframe
          src="https://img.shields.io/github/stars/SuperBruceJia/promptcraft?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe>
        <iframe
          src="https://img.shields.io/github/forks/SuperBruceJia/promptcraft?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe> 
      </li>
    </ul>
  </li>
  
  <li>ğ— ğ—®ğ—¶ğ—»ğ˜ğ—®ğ—¶ğ—»ğ—²ğ—± ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ ğ—¥ğ—²ğ—½ğ—¼ ğ—³ğ—¼ğ—¿ ğ—Ÿğ—¶ğ˜ğ—²ğ—¿ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—¦ğ˜‚ğ—¿ğ˜ƒğ—²ğ˜†<br>
    <ul class="arrow-list">
      Welcome to contribute and work together!
      <li><a href="https://github.com/SuperBruceJia/Awesome-Large-Vision-Language-Model">Awesome Large Vision-Language Model (VLM)</a></li>
      <li><a href="https://github.com/SuperBruceJia/Awesome-Mixture-of-Experts">Awesome Mixture of Experts (MoE)</a></li>
      <li><a href="https://github.com/SuperBruceJia/Awesome-LLM-Self-Consistency">Awesome LLM Self-Consistency</a></li>
      <li><a href="https://github.com/SuperBruceJia/Awesome-Text-Generation-Evaluation">Awesome Text/Natural Language Generation (NLG) Evaluation</a></li>
      <li><a href="https://github.com/SuperBruceJia/Awesome-Semantic-Textual-Similarity">Awesome Semantic Textual Similarity (STS)</a></li>
    </ul>
  </li>

  <br>
  
  <details>
    <summary style="outline:none">Research Focus Illustration</summary>
    <div style="text-align:center">
      <img width="99%device-width" src="./imgs/Multimodal/Multimodal-1.jpg" />
      <img width="99%device-width" src="./imgs/Multimodal/Multimodal-2.jpg" />
      <img width="99%device-width" src="./imgs/Multimodal/Multimodal-3.jpg" />
    </div>
  </details>
  
  <details>
      <summary style="outline:none">Research Presentations and Resources</summary>
      
      <br><a style="color:crimson;">ğ–ğ¡ğšğ­ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğ¨ğ¬ğ­ ğ¬ğ®ğ«ğ©ğ«ğ¢ğ¬ğ¢ğ§ğ  ğŸğ¢ğ§ğğ¢ğ§ğ  ğ¢ğ§ ğ²ğ¨ğ®ğ« ğ«ğğ¬ğğšğ«ğœğ¡?</a>
      
      <p><font color="#9933CC">ğğ«ğğ¬ğğ§ğ­ğšğ­ğ¢ğ¨ğ§</font> - Artificial Intelligence (AI)</p>
        <li><a href="./files/Presentation/Current-Artificial-Intelligence-2023-ShuyueJIA.pptx"> <i class="fa fa-file-powerpoint-o"></i> AI In the 2020s And Beyond</a></li>
      
      <p><font color="#9933CC">ğğ«ğğ¬ğğ§ğ­ğšğ­ğ¢ğ¨ğ§</font> - Graph Neural Network (GNN)</p>
        <li><a href="./files/EEG/GCNs-Net-Summary.pdf"> <i class="fab fa fa-file-pdf-o"></i> Graph Convolutional Neural Networks</a></li>
        <li><a href="./files/EEG/BiLSTM-GCN-Summary.pdf"> <i class="fab fa fa-file-pdf-o"></i> Attention-based BiLSTM-GCN</a></li>
        <li><a href="./files/EEG/Dynamic-GCN-Survey.pdf"> <i class="fab fa fa-file-pdf-o" style="text-decoration:none"></i> Dynamic Graph Convolutional Neural Networks</a></li>
      
      <p><font color="#9933CC">ğğ«ğğ¬ğğ§ğ­ğšğ­ğ¢ğ¨ğ§</font> - Natural Language Processing (NLP)</p>
        <li><a href="./files/Presentation/Foundation-Models-Sequential-Decision-Making.pdf"> <i class="fab fa fa-file-pdf-o"></i> Foundation Models for Sequential Decision Making</a></li>
        <li><a href="./files/Presentation/Factual-Associations-LLMs.pdf"> <i class="fab fa fa-file-pdf-o"></i> Factual Associations in LLMs</a></li>
        <li><a href="./files/Graph-Matching-Paper-Survey.pdf"> <i class="fab fa fa-file-pdf-o"></i> Graph Matching</a></li>
        <li><a href="./files/NMT-Subword-Unites.pdf"> <i class="fab fa fa-file-pdf-o"></i> Sub-word BPE Algorithm for NMT</a></li>
        <li><a href="./files/Concept-Matching.pdf"> <i class="fab fa fa-file-pdf-o"></i> Concept Matching for Medical Terms</a></li>
      
      <p><font color="#9933CC">ğğ«ğğ¬ğğ§ğ­ğšğ­ğ¢ğ¨ğ§</font> - Computer Vision (CV)</p>
        <li><a href="./files/Presentation/IQA-Perceptual-Optimization.pdf"> <i class="fab fa fa-file-pdf-o"></i> Image Quality Assessment and Perceptual Optimization</a></li>
        <li><a href="./files/Model-Compression-Acceleration.pdf"> <i class="fab fa fa-file-pdf-o"></i> Deep Learning Models Compression and Acceleration</a></li>
        <li><a href="./files/Spatial-Sparse-CNNs.pdf"> <i class="fab fa fa-file-pdf-o"></i> 3D Human Pose Estimation and Human Body Reconstruction</a></li>
        <li><a href="./files/YOLO.pdf"> <i class="fab fa fa-file-pdf-o"></i> YOLO Object Detection</a></li>
      
      <p><font color="#9933CC">ğğ«ğğ¬ğğ§ğ­ğšğ­ğ¢ğ¨ğ§</font> - Tutorials and Useful Coding Scripts</p>
        <li><a href="./files/Server.pdf"> <i class="fab fa fa-file-pdf-o"></i> Usage of Cloud Server and Setting-up</a></li>
        <li><a href="./files/Python.pdf"> <i class="fab fa fa-file-pdf-o"></i> Python Environment Setting-up</a></li>
        <li><a href="./files/TensorFlow.pdf"> <i class="fab fa fa-file-pdf-o"></i> TensorFlow for Deep Learning</a></li>
        <li><a href="./files/Crypto-currency-Return-and-Price-Prediction-with-Machine-Learning.pdf"><i class="fab fa fa-file-pdf-o"></i> Crypto Currency Return and Price Prediction with Machine Learning</a></li>
        <li><a href="./files/PySpark-and-Horovod.pdf"> <i class="fab fa fa-file-pdf-o"></i> Big Data Parallel Processing by PySpark and Horovod Distributed Deep Learning</a></li>
        <li><a href="https://github.com/SuperBruceJia/Sci-Hub-Paper-Download-shell"> <i class="fab fa-github"></i> Download Papers from Sci-Hub via Unix Shell</a></li>
        <li><a href="https://github.com/SuperBruceJia/Google-Scholar-Citations-Download"> <i class="fab fa-github"></i> Retrieve and Download Google Scholar Citation Papers</a></li>
        <li><a href="https://github.com/SuperBruceJia/dynamic-web-crawlering-python"> <i class="fab fa-github"></i> Dynamic (Ajax) Web Crawler in Python</a></li>
        <li><a href="https://github.com/SuperBruceJia/pytorch-flask-deploy-webapp"> <i class="fab fa-github"></i> Deploy Machine Learning and Deep Learning Models with Flask and Docker as Web Applications</a></li>
        <li><a href="https://github.com/SuperBruceJia/CityU-MPhil-Thesis"> <i class="fab fa-github"></i> M.Phil. Thesis (LaTeX), City University of Hong Kong</a></li>

      <p><font color="#9933CC">ğ„ğ±ğœğğ¥ğ¥ğğ§ğ­ ğğ«ğğ¬ğğ§ğ­ğšğ­ğ¢ğ¨ğ§</font></p>
        <li><a href="./resources/PowerPoint/On%20the%20inductive%20bias%20of%20language%20modeling.pptx"> <i class="fa fa-file-powerpoint-o" style="font-size:24px;color:red"></i> On the Inductive Bias of Language Modeling</a> | <a href="https://youtu.be/PNTbvoweqBk?t=3046"> <i class="fa fa-youtube-play" style="font-size:24px;color:red"></i> Oral Presentation (Tatsunori B. Hashimoto)</a></li>
      
      <p><font color="#9933CC">ğ“ğğœğ¡ğ§ğ¢ğœğšğ¥ ğğ¨ğ¨ğ¤ğ¬</font></p>
        <li><a href="./books/Foundation-Models-for-Natural-Language-Processing.pdf"> <i class="fab fa fa-book"></i> Foundation Models for Natural Language Processing: Pre-trained Language Models Integrating Media (Gerhard PaaÃŸ and Sven Giesselbach)</a></li>
        <li><a href="./books/The-Path-to-Artificial-General-Intelligence.pdf"> <i class="fab fa fa-book"></i> The Path to Artificial General Intelligence: Insights from Adversarial LLM Dialogue (Edward Y. Chang)</a></li>
        <li><a href="./books/Pattern%20Recognition%20and%20Machine%20Learning.pdf"> <i class="fab fa fa-book"></i> Pattern Recognition and Machine Learning (Christopher Bishop)</a></li>
        <li><a href="./books/Reinforcement%20Learning-%20An%20Introduction.pdf"> <i class="fab fa fa-book"></i> Reinforcement Learning: An Introduction (Richard S. Sutton and Andrew G. Barto)</a></li>
        <li><a href="./books/Digital%20Image%20Processing.pdf"> <i class="fab fa fa-book"></i> Digital Image Processing (Rafael C. Gonzalez and Richard E. Woods)</a></li>
        <li><a href="./books/Multiple%20View%20Geometry%20in%20Computer%20Vision.pdf"> <i class="fab fa fa-book"></i> Multiple View Geometry in Computer Vision (Richard Hartley and Andrew Zisserman)</a></li>
        <li><a href="./books/Graph%20Representation%20Learning.pdf"> <i class="fab fa fa-book"></i> Graph Representation Learning (William L. Hamilton)</a></li>
        <li><a href="./books/Computer%20Systems-%20A%20Programmer's%20Perspective.pdf"> <i class="fab fa fa-book"></i> Computer Systems: A Programmer's Perspective (Randal E. Bryant and David R. O'Hallaron)</a></li>
        <li><a href="./books/Computer%20Organization%20and%20Design-%20The%20Hardware%3ASoftware%20Interface.pdf"> <i class="fab fa fa-book"></i> Computer Organization and Design: The Hardware/Software Interface (David A. Patterson and John L. Hennessy)</a></li>
      
      <p><font color="#9933CC">ğ€ğœğšğğğ¦ğ¢ğœ ğ‘ğğ¬ğ¨ğ®ğ«ğœğğ¬</font></p>
        <li><a href="./resources/Writing-and-Technical-Presentations.pdf"> <i class="fab fa fa-file-pdf-o"></i> Writing and Technical Presentations by Prof. Ayse Coskun</a></li>
        <li><a href="./resources/English-Writing.pdf"> <i class="fab fa fa-file-pdf-o"></i> The Most Common Habits from more than 200 English Papers written by Graduate Chinese Engineering Students by Felicia Brittman</a></li>
        <li><a href="./resources/How_to_Write_Good_Research_Articles_Xiaohua_JIA.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to Write Good Research Articles by Prof. Xiaohua Jia</a></li>
        <li><a href="./resources/ä¸šä½™åšç ”ç©¶çš„ç»éªŒ-ç”°æ¸Šæ ‹.pdf"> <i class="fab fa fa-file-pdf-o"></i> ä¸šä½™åšç ”ç©¶çš„ç»éªŒ by Dr. Yuandong Tian</a></li>
        <li><a href="./resources/How_to_do_Research.ppt"> <i class="fa fa-file-powerpoint-o" style="font-size:24px;color:red"></i> What is Research and How to do it? by Prof. Yi Ma</a></li>
        <li><a href="./resources/CVPR_Submission.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to Write a Good CVPR Submission? by Prof. Bill Freeman</a></li>
        <li><a href="./resources/Tips-on-Writing-Papers-with-Mathematical-Content.pdf"> <i class="fab fa fa-file-pdf-o"></i> Tips on Writing Papers with Mathematical Content by Prof. John N. Tsitsiklis</a></li>
        <li><a href="./resources/How_to_Use_IEEEtran_LaTeX_Class.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to Use the IEEEtran LaTeX Class</a></li>
        <li><a href="./resources/IEEE-Editing-Mathematics.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Editing Mathematics Guide (Standard)</a></li>
        <li><a href="./resources/IEEE_Math_Typesetting_Guide.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Math Typesetting Guide (LaTeX)</a></li>
        <li><a href="./resources/Formula_Comma_IEEE.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Formula Comma and Period</a></li>
        <li><a href="./resources/IEEE-Reference-Guide-2021.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Reference Guide (2021)</a></li>
        <li><a href="./resources/IEEE_Citation_Examples.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Citation Examples</a></li>
        <li><a href="https://woodward.library.ubc.ca/research-help/journal-abbreviations/"> <i class="fab fa-internet-explorer"></i> Science and Engineering Journal Abbreviations</a></li>
        <li><a href="https://libguides.uwf.edu/c.php?g=848362&p=6073747"> <i class="fab fa-internet-explorer"></i> Standard abbreviations used in the IEEE Reference list</a></li>
        <li><a href="./resources/IEEE-Editorial-Style-Manual-2021.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Editorial Style Manual (2021)</a></li>
        <li><a href="./resources/LATEX-Mathematical-Symbols.pdf"> <i class="fab fa fa-file-pdf-o"></i> LaTeX Mathematical Symbols</a></li>
        <li><a href="./resources/Typesetting-Subtleties-Prof-Vivek-Goyal.pdf"> <i class="fab fa fa-file-pdf-o"></i> Typesetting Subtleties by Prof. Vivek Goyal</a></li>
        <li><a href="./resources/Markdown_Mathematics.pdf"> <i class="fab fa fa-file-pdf-o"></i> Writing Mathematical Formulas in Markdown</a></li>
        <li><a href="./resources/JCR_2021.pdf"> <i class="fab fa fa-file-pdf-o"></i> InCites Journal Citation Reports (2021)</a></li>
        <li><a href="./resources/ä¸­å›½è®¡ç®—æœºå­¦ä¼šCCFæ¨èå›½é™…å­¦æœ¯ä¼šè®®å’ŒæœŸåˆŠç›®å½•_2019.pdf"> <i class="fab fa fa-file-pdf-o"></i> ä¸­å›½è®¡ç®—æœºå­¦ä¼šæ¨èå›½é™…å­¦æœ¯ä¼šè®®å’ŒæœŸåˆŠç›®å½• (2019)</a></li>
        <li><a href="./resources/ä¸­å›½ç§‘å­¦é™¢SCIåˆ†åŒºè¡¨_2019.xls"> <i class="fa fa-file-excel-o" style="font-size:24px;color:red"></i> ä¸­å›½ç§‘å­¦é™¢SCIåˆ†åŒºè¡¨ (2019)</a></li>
        <li><a href="https://earlywarning.fenqubiao.com/#/README"> <i class="fab fa-internet-explorer"></i> ä¸­å›½ç§‘å­¦é™¢å›½é™…æœŸåˆŠé¢„è­¦åå•</a></li>
        <li><a href="https://github.com/SuperBruceJia/CVPR-LaTeX-Paper-Template"> <i class="fab fa-github"></i> CVPR Paper, Supplementary Materials, and Rebuttal LaTeX Templates</a></li>
        <li><a href="https://github.com/SuperBruceJia/Poster_Template"> <i class="fab fa-github"></i> PowerPoint (PPT) and LaTeX Demos of Academic Posters</a></li>
        <li><a href="https://aiindex.stanford.edu/report/"> <i class="fab fa-internet-explorer"></i> Artificial Intelligence Index Report by Stanford</a></li>
        <li><a href="https://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html"> <i class="fab fa-internet-explorer"></i> Defining the Role of Authors and Contributors by International Committee of Medical Journal Editors</a></li>
        <li><a href="./resources/Instruction_to_Reviewers_IEEE-TMI.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to review a paper? by IEEE T-MI</a></li>
        <li><a href="./resources/How-to-be-a-good-Meta-Reviewer.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to be a Good Meta-reviewer? by ICML 2022 Program Chairs</a></li>
        <li><a href="./resources/How_Junior_Faculty_Can_Secure_Industry_Funding.pdf"> <i class="fab fa fa-file-pdf-o"></i> How Junior Faculty Can Secure Industry Funding? by Prof. Zhou Yu</a></li>

      <p><font color="#9933CC">ğ‘ğğœğ¨ğ¦ğ¦ğğ§ğğğ ğ‘ğğšğğ¢ğ§ğ ğ¬</font></p>
        <li><a href="./resources/Research_Life.pdf"> <i class="fab fa fa-file-pdf-o"></i> æ–‡ç« åƒå¤äº‹, å¾—å¤±å¯¸å¿ƒçŸ¥ by Prof. Song-Chun Zhu</a></li>
        <li><a href="./resources/Statement_of_Purpose-Kai-Fu-LEE-CMU.pdf"> <i class="fab fa fa-file-pdf-o"></i> Statement of Purpose by Dr. Kai-Fu Lee</a></li>
        <li><a href="./resources/AI-Dartmouth-College-1956.pdf"> <i class="fab fa fa-file-pdf-o"></i> Dartmouth Summer Project on Artificial Intelligence (1956)</a></li>
        <li><a href="./resources/The_Summer_Vision_Project_1966.pdf"> <i class="fab fa fa-file-pdf-o"></i> The Summer Vision Project at MIT (1966)</a></li>
        <li><a href="./resources/Turing-Test-1950.pdf"> <i class="fab fa fa-file-pdf-o"></i> Turing Test (1950) by Alan Mathison Turing</a></li>
        <li><a href="./resources/The-Bitter-Lesson.pdf"> <i class="fab fa fa-file-pdf-o"></i> The Bitter Lesson by Rich Sutton</a></li>
        <li><a href="./resources/Annotated-History-of-Modern-AI-and-Deep-Learning.pdf"> <i class="fab fa fa-file-pdf-o"></i> Annotated History of Modern AI and Deep Learning by JÃ¼rgen Schmidhuber</a></li>
        <li><a href="./resources/Who-invented-convolutional-neural-networks.pdf"> <i class="fab fa fa-file-pdf-o"></i> Who invented convolutional neural networks? by JÃ¼rgen Schmidhuber</a></li>
        <li><a href="./resources/Academic-Achievements-Richard.pdf"> <i class="fab fa fa-file-pdf-o"></i> Academic Achievements of Prof. Richard Yi-Da Xu</a></li>
        <li><a href="./resources/Bio-Richard.pdf"> <i class="fab fa fa-file-pdf-o"></i> Brief Bio of Prof. Richard Yi-Da Xu</a></li>
        <li><a href="./resources/The-PhD-Grind.pdf"> <i class="fab fa fa-file-pdf-o"></i> The Ph.D. Grind by Prof. Philip J. Guo</a></li>
        <li><a href="./resources/You-and-Your-Research.pdf"> <i class="fab fa fa-file-pdf-o"></i> You and Your Research by Prof. Richard Hamming</a></li>
        <li><a href="https://news.stanford.edu/2005/06/12/youve-got-find-love-jobs-says/"> <i class="fab fa-internet-explorer"></i> Youâ€™ve Got to Find What You Love by Steve Jobs</a></li>
        <li><a href="./resources/Computer_Vision_History.pdf"> <i class="fab fa fa-file-pdf-o"></i> A Brief History of Computational Vision by Prof. Song-Chun Zhu</a></li>
        <li><a href="./resources/Artificial_Intelligence.pdf"> <i class="fab fa fa-file-pdf-o"></i> Artificial Intelligence by Prof. Song-Chun Zhu</a></li>
        <li><a href="./resources/ä¸Šæµ·äº¤é€šå¤§å­¦å­¦ç”Ÿç”Ÿå­˜æ‰‹å†Œ.pdf"> <i class="fab fa fa-file-pdf-o"></i> ä¸Šæµ·äº¤é€šå¤§å­¦å­¦ç”Ÿç”Ÿå­˜æ‰‹å†Œ</a></li>
        <li><a href="https://www.youtube.com/watch?v=ji5_MqicxSo"> <i class="fa fa-youtube-play" style="font-size:24px;color:red"></i> Randy Pausch Last Lecture: Achieving Your Childhood Dreams</a></li>
  </details>
  
  <details>
    <summary style="outline:none">Teaching Materials</summary>
    <p><font color="#9933CC">ENG EC 327: Introduction to Software Engineering, Boston University (Spring 2025)</font></p>
    <li><a href="./teaching/BU-Spring-2025-ECE327-Introduction-to-Software-Engineering/EC327-Lab-Session-1.pdf"> <i class="fab fa fa-file-pdf-o"></i> Lab Session 1: Linux/Unix Tutorial - Your First Technical Course for Industry Preparation</a></li>
    <li><a href="./teaching/BU-Spring-2025-ECE327-Introduction-to-Software-Engineering/EC327-Lab-Session-2.pdf"> <i class="fab fa fa-file-pdf-o"></i> Lab Session 2: Programming in C/C++: A Hands-on Introduction</a></li>
    <li><a href="./teaching/BU-Spring-2025-ECE327-Introduction-to-Software-Engineering/EC327-Lab-Session-4.pdf"> <i class="fab fa fa-file-pdf-o"></i> Lab Session 4: GitHub: Building Your Coding Profile</a></li>
    <li><a href="./teaching/BU-Spring-2025-ECE327-Introduction-to-Software-Engineering/EC327-Lab-Session-7.pdf"> <i class="fab fa fa-file-pdf-o"></i> Lab Session 7: Unit Testing in Android with GitHub Action</a></li>
  </details>
  
  <details>
    <summary style="outline:none">Contact and Our Team</summary>
    <p><font color="#9933CC">ğ—–ğ—¼ğ—»ğ˜ğ—®ğ—°ğ˜</font></p>
    Shuyue Jia (Bruce Jia)<br>
    Department of ECE, Boston University<br>
    Add: Kolachalama Lab, 14/F, Center for Computing & Data Sciences,<br>
    Boston University, 665 Commonwealth Ave., Boston, MA 02215<br>
    Tel: +1 (617) 685-1479<br>
    Email: <a href="mailto:brucejia@bu.edu">brucejia@bu.edu</a>; <a href="mailto:shuyuej@ieee.org">shuyuej@ieee.org</a><br>
    
    <p><font color="#9933CC">ğ—¥ğ—²ğ˜€ğ—²ğ—®ğ—¿ğ—°ğ—µ ğ—§ğ—²ğ—®ğ—º</font></p>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/team/Research_at_BU_2024_Spring.jpg" /><br><br>
      <p class="image-description">Our Research Team @ <a href="https://vkola-lab.github.io/team/">Kolachalama Laboratory</a></p>
      <p class="image-description">BU Center for Computing & Data Sciences, Boston, MA (Spring 2024)</p>
    </div>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/team/Research_at_BU_2024_Fall.jpg" /><br><br>
      <p class="image-description">Our Research Team @ <a href="https://vkola-lab.github.io/team/">Kolachalama Laboratory</a></p>
      <p class="image-description">BU Center for Computing & Data Sciences, Boston, MA (Fall 2024)</p>
    </div>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/team/Research_at_BU_2024_Winter_1.jpg" /><br><br>
      <p class="image-description">Our Research Team @ <a href="https://vkola-lab.github.io/team/">Kolachalama Laboratory</a></p>
      <p class="image-description">BU Center for Computing & Data Sciences, Boston, MA (Winter 2024)</p>
    </div>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/team/Research_at_BU_2024_Winter_2.jpg" /><br><br>
      <p class="image-description">Our Research Team @ <a href="https://vkola-lab.github.io/team/">Kolachalama Laboratory</a></p>
      <p class="image-description">BU Center for Computing & Data Sciences, Boston, MA (Winter 2024)</p>
    </div>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/team/Research_at_BU_2025_Fall.jpg" /><br><br>
      <p class="image-description">Our Research Team @ <a href="https://vkola-lab.github.io/team/">Kolachalama Laboratory</a></p>
      <p class="image-description">BU Center for Computing & Data Sciences, Boston, MA (Winter 2025)</p>
    </div>
    
    <p><font color="#9933CC">ğ—œğ—˜ğ—˜ğ—˜ ğ—§ğ—²ğ—®ğ—º</font></p>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/IEEE/IEEE_2025.png" /><br><br>
      <p class="image-description">Our IEEE Local Conference Committee Team @ <a href="https://ieeeboston.org">IEEE Boston Section</a></p>
      <p class="image-description">Joyce Cummings Center, School of Engineering, Tufts University, 177 College Avenue, Medford, MA (Spring 2025)</p>
    </div>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/IEEE/IEEE_2024.JPG" /><br><br>
      <p class="image-description">Our IEEE Local Conference Committee Team @ <a href="https://ieeeboston.org">IEEE Boston Section</a></p>
      <p class="image-description">Crowne Plaza, 15 Canal Park, Woburn, MA (Fall 2024)</p>
    </div>
    
  </details>
  
  <br>
  
  Email: <a href="mailto:brucejia@bu.edu">brucejia@bu.edu</a>; <a href="mailto:shuyuej@ieee.org">shuyuej@ieee.org</a>

  <br>
  <a href="./files/Resume/Curriculum_Vitae_Shuyue_JIA.pdf"><i class="fab fa fa-file-pdf-o"></i>&#32;Resume</a>
  <a href="https://scholar.google.com/citations?user=PfpEP60AAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-1x"></i>&#32;Scholar</a>
  <a href="https://huggingface.co/shuyuej">&#129303;&#32;HuggingFace</a>
  <a href="https://github.com/SuperBruceJia"><i class="fab fa-github"></i>&#32;GitHub</a>
  <iframe src="https://img.shields.io/github/followers/SuperBruceJia?label=follow&style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
  <iframe src="https://img.shields.io/github/stars/SuperBruceJia?label=stars&style=social" frameborder="0" scrolling="0" width="90px" height="20px"></iframe>
</p>

<h2 id="news">News</h2>
<ul style="list-style:none; margin:0; padding:0">
  <li>
    <strong>Jan 2026</strong> Our <a href="https://www.sciencedirect.com/science/article/pii/S1386505626000791"> Medical Agentic RAG System</a> is accepted by <em>International Journal of Medical Informatics</em>!
  </li>
  <li>
    <strong>Jan 2026</strong> I will serve as a Program Committee member for ACL 2026 (Industry Track).
  </li>
  <li>
    <strong>Nov 2025</strong> I was appointed as the Area Chair for the IEEE Boston Section Circuits and Systems (CAS) Society.
  </li>
  <li>
    <strong>Oct 2025</strong> I will serve as a Technical Track Co-chair for IEEE MIT URTC 2025.
  </li>
  <li>
    <strong>Aug 2025</strong> I will serve as the Speaker Program Chair (Keynotes & Panel Sessions) and a Steering Committee member for IEEE ICAD 2026.
  </li>
  <li>
    <strong>Aug 2025</strong> Our paper <a href="https://aclanthology.org/2025.emnlp-main.893">Multi-agent System for Document Intelligence</a> is accepted by <em>EMNLP 2025</em>.
  </li>
  <li>
    <strong>Aug 2025</strong> A <a href="./files/Presentation/Evidence-Retrieval-and-Grounding.pdf"> Presentation</a> of Evidence Retrieval and Grounding in Medicine.
  </li>
  <li>
    <strong>Aug 2025</strong> Our <a href="https://www.medrxiv.org/content/10.1101/2025.08.06.25333160v1"> Medical Agentic RAG System</a> is available online!
  </li>
  <li>
    <strong>July 2025</strong> Open Source <a href="https://github.com/SuperBruceJia/Awesome-Text-Generation-Evaluation"> Awesome Text Generation Evaluation</a>, a curated list of evaluation metrics for Natural Language Generation (NLG)
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-Text-Generation-Evaluation?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-Text-Generation-Evaluation?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>May 2025</strong> Our paper <a href="https://www.nature.com/articles/s44385-025-00022-0">PodGPT</a> is accepted by <em>Nature npj Biomedical Innovations</em>.
  </li>
  <li>
    <strong>Mar 2025</strong> Honored to receive the ğŸ®ğŸ¬ğŸ®ğŸ± ğ—œğ—˜ğ—˜ğ—˜ ğ—•ğ—¼ğ˜€ğ˜ğ—¼ğ—» ğ—¦ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—”ğ—¿ğ˜ğ—µğ˜‚ğ—¿ ğ—ªğ—¶ğ—»ğ˜€ğ˜ğ—¼ğ—» ğ—¦ğ˜ğ˜‚ğ—±ğ—²ğ—»ğ˜ ğ—”ğ˜„ğ—®ğ—¿ğ—± from the <a href="https://ieeeboston.org/">IEEE Boston Section</a>, <em>"For outstanding contributions to the Boston Section Local Conference Committee"</em>.
  </li>
  <li>
    <strong>Dec 2024</strong> I will serve as a Scientific Program Committee member for AMIA 2025 Clinical Informatics Conference.
  </li>
  <li>
    <strong>Dec 2024</strong> We have released the source code for our <a href="https://github.com/vkola-lab/PodGPT/tree/main/rag_pipeline"> PodGPT with Retrieval-Augmented Generation (RAG)</a>. This powerful pipeline allows our model to ground responses with the latest scientific evidence from top-tier literature, such as <em>The New England Journal of Medicine</em> (NEJM)!
  </li>
  <li>
    <strong>Nov 2024</strong> Our <a href="https://www.nature.com/articles/s44385-025-00022-0"> PodGPT preprint</a> is available online! It is an audio-augmented Large Language Model (LLM) for STEMM research and education.
  </li>
  <li>
    <strong>Oct 2024</strong> We have launched <b>PodRAG</b> on our <a href="https://podgpt.org/">PodGPT</a> platform with the advanced ğ—¥ğ—²ğ˜ğ—¿ğ—¶ğ—²ğ˜ƒğ—®ğ—¹-ğ—”ğ˜‚ğ—´ğ—ºğ—²ğ—»ğ˜ğ—²ğ—± ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» (ğ—¥ğ—”ğ—š) techniques! It is designed to provide ğ˜ğ—µğ—² ğ—ºğ—¼ğ˜€ğ˜ ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ˜ğ—² ğ—®ğ—»ğ—± ğ˜‚ğ—½-ğ˜ğ—¼-ğ—±ğ—®ğ˜ğ—² ğ—¶ğ—»ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ˜ğ—¶ğ—¼ğ—» for medical education and research! Please try it out if you are interested!
  </li>
  <li>
    <strong>Aug 2024</strong> I will serve as the Speaker Program Chair (Keynotes & Panel Sessions) and a Steering Committee member for IEEE ICAD 2025.
  </li>
  <li>
    <strong>Aug 2024</strong> Open Source and Keep Updating <a href="https://github.com/SuperBruceJia/Awesome-Large-Vision-Language-Model"> Awesome Large Vision-Language Model (LVLM/MM-LLM)</a>, a curated list of Large Vision-Language Model
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-Large-Vision-Language-Model?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-Large-Vision-Language-Model?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>, and <a href="https://github.com/SuperBruceJia/Awesome-Mixture-of-Experts"> Awesome Mixture of Experts (MoE)</a>, a curated list of Mixture of Experts (MoE) and Mixture of Multimodal Experts (MoME)
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-Mixture-of-Experts?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-Mixture-of-Experts?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
    Welcome to contribute and work together!
  </li>
  <li>
    <strong>Aug 2024</strong> Release and maintain a collection of <a href="https://huggingface.co/collections/shuyuej/quantization-669ea25d2ea444924e543da2"> &#129303;&#32; Quantized Large Language Models</a> for public usage, offering AI solutions with reduced computational requirements.
  </li>
  <li>
    <strong>July 2024</strong> Open Source <a href="https://github.com/vkola-lab/PodGPT"> PodGPT Library</a>, a library for benchmarking multilingual medical Large Language Models (Medical LLMs)
    <iframe
      src="https://img.shields.io/github/stars/vkola-lab/PodGPT" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/vkola-lab/PodGPT" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>July 2024</strong> Our <a href="https://www.medrxiv.org/content/10.1101/2024.07.11.24310304v1"> MedPodGPT preprint</a> is available online! It is an audio-augmented Large Language Model (LLM) for medical research and education.
  </li>
  <li>
    <strong>June 2024</strong> Our AI Platform, <a href="https://podgpt.org/"> PodGPT</a>, is now accessible to the general public. It is an online platform for deploying our latest multimodal foundation models for education and research.
  </li>
  <li>
    <strong>June 2024</strong> Our paper <a href="https://ieeexplore.ieee.org/document/10566053"> MedSyn</a> is accepted by <em>IEEE T-MI</em>.
  </li>
  <li>
    <strong>May 2024</strong> ğ—£ğ—µ.ğ——. ğ—–ğ—®ğ—»ğ—±ğ—¶ğ—±ğ—®ğ—°ğ˜† ğ—¥ğ—²ğ—½ğ—¼ğ—¿ğ˜: <a href="./files/PhD_Candidacy/PhD_Candidacy_Report_Shuyue_Jia_May_2024.pdf">Preference Alignment via Reinforcement Learning from Human Feedback</a> and ğ—£ğ—¿ğ—²ğ˜€ğ—²ğ—»ğ˜ğ—®ğ˜ğ—¶ğ—¼ğ—»: <a href="./files/PhD_Candidacy/PhD_Candidacy_Presentation_Shuyue_Jia_May_2024.pdf">Slides</a>.
  </li>
  <li>
    <strong>Jan 2024</strong> ğŸ”¥ We are releasing <a href="https://huggingface.co/datasets/shuyuej/GSM8K-Consistency">&#129303;&#32;GSM8K-Consistency</a>, a benchmark database for analyzing the consistency of Arithmetic Reasoning on GSM8K.
  </li>
  <li>
    <strong>Dec 2023</strong> Open Source <a href="https://github.com/SuperBruceJia/promptcraft"> ğŸ”¨ PromptCraft</a> and its published <a href="https://pypi.org/project/promptcraft"> PyPI Package</a>, a prompt perturbation toolkit from the character, word, and sentence levels for prompt robustness analysis
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/promptcraft?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/promptcraft?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Oct 2023</strong> Open Source <a href="https://github.com/SuperBruceJia/Awesome-Semantic-Textual-Similarity"> Awesome Semantic Textual Similarity</a>, a curated list of Semantic/Sentence Textual Similarity (STS) in Large Language Models (LLMs) and Natural Language Processing (NLP)
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-Semantic-Textual-Similarity?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-Semantic-Textual-Similarity?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Oct 2023</strong> A <a href="./files/Presentation/Sentence_Textual_Similarity-Model_Evolution_Overview.pdf"> Presentation</a> of Sentence Textual Similarity: Model Evolution Overview.
  </li>
  <li>
    <strong>Oct 2023</strong> Open Source <a href="https://github.com/SuperBruceJia/Awesome-LLM-Self-Consistency"> Awesome LLM Self-Consistency</a>, a curated paper and presentation list of self-consistency in Large Language Models (LLMs)
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-LLM-Self-Consistency?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-LLM-Self-Consistency?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Oct 2023</strong> A <a href="./files/Presentation/Prompt-Perturbation-and-Robustness-Evaluation-2023-Shuyue-JIA.pdf"> Presentation</a> of Prompt Perturbation and Robustness Evaluation.
  </li>
  <li>
    <strong>Sept 2023</strong> A <a href="./files/Presentation/Prompt-based-Learning-and-Robustness-Evaluation-2023-Shuyue-JIA.pdf"> Presentation</a> of Prompt-based Learning and Robustness Evaluation.
  </li>
  <li>
    <strong>Sept 2023</strong> A <a href="./files/Presentation/Self-Consistency-Benefits-Large-Language-Models-2023-Shuyue-JIA.pdf"> Presentation</a> of Self-Consistency Benefits Large Language Models.
  </li>
  <li>
    <strong>Sept 2023</strong> Our paper <a href="https://ieeexplore.ieee.org/abstract/document/10251073/"> Deep Transfer Learning</a> is accepted by <em>IEEE T-IM</em>.
  </li>
  <li>
    <strong>May 2023</strong> ğ— .ğ—£ğ—µğ—¶ğ—¹.ğ—§ğ—µğ—²ğ˜€ğ—¶ğ˜€: <a href="https://scholars.cityu.edu.hk/en/theses/noreference-image-quality-assessment-via-nonlocal-modeling(2d1e72fb-2405-43df-aac9-4838b6da1875).html"> No-reference Image Quality Assessment via Non-local Modeling</a> and ğ——ğ—²ğ—³ğ—²ğ—»ğ˜€ğ—² ğ—¦ğ—¹ğ—¶ğ—±ğ—²ğ˜€: <a href="./files/Thesis/MPhil-Thesis-Defense-Presentation-ShuyueJia.pdf"> Image Quality Assessment and Perceptual Optimization: A Non-local Modeling Approach</a>.
  </li>
  <li>
    <strong>Mar 2023</strong> A <a href="./files/Presentation/Foundation-Models-Sequential-Decision-Making.pdf"> Presentation</a> of Foundation Models for Sequential Decision Making.
  </li>
  <li>
    <strong>Jan 2023</strong> A <a href="./files/Presentation/A_Summary_Three_Projects.pdf"> Presentation</a> of IQA Regression and EEG Classification.
  </li>
  <li>
    <strong>Nov 2022</strong> A <a href="./files/Proposal/Research_Proposal_VPS.pdf"> Research Proposal</a> of Video Panoptic Segmentation (VPS).
  </li>
  <li>
    <strong>Aug 2022</strong> Our paper <a href="https://ieeexplore.ieee.org/document/9889159"> GCNs-Net</a> is accepted by <em>IEEE T-NNLS</em>
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/EEG-DL?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/EEG-DL?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Aug 2022</strong> Our paper <a href="https://ieeexplore.ieee.org/abstract/document/9950035"> NLNet</a> is accepted by <em>IEEE MMSP</em>. Source codes are available on <a href="https://github.com/SuperBruceJia/NLNet-IQA">&#32;GitHub</a>
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/NLNet-IQA?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/NLNet-IQA?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>, and the trained models are available on <a href="https://huggingface.co/shuyuej/NLNet">&#129303;&#32;HuggingFace</a> for real-life IQA inference.
  </li>
  <li>
    <strong>Dec 2021</strong> Our paper <a href="https://www.frontiersin.org/article/10.3389/fbioe.2021.706229"> BiLSTM-GCNs</a> is accepted by <em>Frontiers in Bioengineering and Biotechnology</em>.
  </li>
  <li>
    <strong>Apr 2020</strong> Open Source <a href="https://github.com/SuperBruceJia/EEG-DL"> EEG-DL</a>, a Deep Learning (DL) library written by TensorFlow for EEG Signals Classification
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/EEG-DL?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/EEG-DL?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Feb 2020</strong> Our paper<a href="https://iopscience.iop.org/article/10.1088/1741-2552/ab4af6/meta"> ESI-CNNs</a> is accepted by <em>Journal of Neural Engineering</em> 
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/EEG-Motor-Imagery-Classification-CNNs-TensorFlow?style=social"
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/EEG-Motor-Imagery-Classification-CNNs-TensorFlow?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>.
  </li>
</ul>

<h2 id="publications">Publications</h2>
<h3 id="Generative AI"><font color="black">Topic 1 - </font><font color="#DE3163">Generative AI and Foundation Models</font></h3>
<ol>
  <li>
    <div style="float:left;">
      <font color="#00066f">Agentic Memory-augmented Retrieval and Evidence Grounding for Medical Question-Answering Tasks</font>
    </div>
    <div style="float:right;">
      <a href="./files/Presentation/Evidence-Retrieval-and-Grounding.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="./files/Manuscripts/Tool_using_AI_agent_2025.pdf"><i class="fab fa fa-file-pdf-o"></i> Manuscript</a>
      <a href="https://www.sciencedirect.com/science/article/pii/S1386505626000791"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    <strong>Shuyue Jia</strong>, Subhrangshu Bit, Varuna H. Jasodanand, Yi Liu, Vijaya B. Kolachalama
    <br><em>International Journal of Medical Informatics</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/Agent/Medical-Agent.jpg" alt="Medical AI Agent Project">
          </div>
        </div>
        We developed a unified, open-source LLM-based agentic system that integrates document retrieval, re-ranking, evidence grounding, and diagnosis generation to support dynamic, multi-step medical reasoning. Our system features a lightweight retrieval-augmented generation pipeline coupled with a cache-and-prune memory bank, enabling efficient long-context inference beyond standard LLM limits. The system autonomously invokes specialized tools, eliminating the need for manual prompt engineering or brittle multi-stage templates.
      </ul>
    </details>
  </li>

  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">DocAgent: An Agentic Framework for Multi-Modal Long-Context Document Understanding</font>
    </div>
    <div style="float:right;">
      <a href="https://aclanthology.org/2025.emnlp-main.893"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    Li Sun, Liu He, <strong>Shuyue Jia</strong>, Yangfan He, Chenyu You
    <br><em>The 2025 Conference on Empirical Methods in Natural Language Processing</em> (EMNLP 2025)
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/Agent/DocAgent.jpg" alt="Document AI Agent Project">
          </div>
        </div>
        We introduce DocAgent, a multi-agent framework for long-context document understanding that imitates the human reading practice. Specifically, we first extract a structured, tree-formatted outline from documents to help agents identify relevant sections efficiently. Further, we develop an interactive reading interface that enables agents to query and retrieve various types of content dynamically. To ensure answer reliability, we introduce a reviewer agent that cross-checks responses using complementary sources and maintains a task-agnostic memory bank to facilitate knowledge sharing across tasks. 
      </ul>
    </details>
  </li>
  
  <hr>

  <li>
    <div style="float:left;">
      <font color="#00066f">PodGPT: An Audio-augmented Large Language Model for Research and Education</font>
    </div>
    <div style="float:right;">
      <a href="https://podgpt.org"><i class="fa fa-codepen"></i> Product</a>
      <a href="./files/Presentation/Evidence-Retrieval-and-Grounding.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/vkola-lab/podgpt"><i class="fa fa-github"></i> Codes</a>
      <a href="https://www.nature.com/articles/s44385-025-00022-0"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
      <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs44385-025-00022-0/MediaObjects/44385_2025_22_MOESM1_ESM.pdf"><i class="fab fa fa-file-pdf-o"></i> Supplementary Materials</a>
    </div><br>
    <strong>Shuyue Jia</strong>, Subhrangshu Bit, Edward Searls, Meagan V. Lauber, Lindsey A. Claus, Pengrui Fan, Varuna H. Jasodanand, Divya Veerapaneni, William M. Wang, Rhoda Au, Vijaya B. Kolachalama
    <br><em>Nature npj Biomedical Innovations</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/PodGPT/PodGPT.png" alt="PodGPT Project">
          </div>
        </div>
        Here, we introduce PodGPT, an audio-augmented large language model (LLM) tailored for research and education. The process began by leveraging publicly available generative AI auto-regressive language models across various scales. These models underwent continuous pre-training on a curated dataset of English CC-BY podcasts produced by scientific journals and clinical experts, as well as content from <em>The New England Journal of Medicine</em> (NEJM). The podcast corpus comprised over 3,700 hours of audio, covering diverse topics in science, research, and medicine, visually summarized in the accompanying word cloud. The next phase involved developing the software infrastructure, which included an inference engine for model deployment, a messaging queue, database integration, retrieval augmented generation (RAG) implementation, API microservices, and a responsive human-machine interface. This highly performant and robust system enabled users with internet access to engage seamlessly with current research and educational material via an adaptive chatbot. The chatbot supported multi-turn conversations across various languages, empowering users to access and interact with STEMM knowledge in a dynamic and accessible manner.
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">MedPodGPT: A Multilingual Audio-augmented Large Language Model for Medical Research and Education</font>
    </div>
    <div style="float:right;">
      <a href="https://podgpt.org"><i class="fa fa-codepen"></i> Product</a>
      <a href="./files/Presentation/Evidence-Retrieval-and-Grounding.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/vkola-lab/medpodgpt"><i class="fa fa-github"></i> Codes</a>
      <a href="https://www.medrxiv.org/content/10.1101/2024.07.11.24310304v1"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    <strong>Shuyue Jia</strong>, Subhrangshu Bit, Edward Searls, Lindsey A. Claus, Pengrui Fan, Varuna H. Jasodanand, Meagan V. Lauber, Divya Veerapaneni, William M. Wang, Rhoda Au, Vijaya B. Kolachalama
    <br><em>Technical Report</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/PodGPT/MedPodGPT.png" alt="MedPodGPT Project">
          </div>
        </div>
        Here, we introduce MedPodGPT, an audio-augmented large language model (LLM) designed for medical research and education. Medical podcasts offer audio content rich in specialized terminology, diverse medical topics, and expert dialogues, helping the medical community stay current with the latest information. Integrating this content into LLMs can enhance their ability to provide up-to-date clinical information.
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images</font>
    </div>
    <div style="float:right;">
      <a href="https://github.com/batmanlab/MedSyn"><i class="fa fa-github"></i> Codes</a>
      <a href="https://ieeexplore.ieee.org/document/10566053"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    Yanwu Xu, Li Sun, Wei Peng, <strong>Shuyue Jia</strong>, Katelyn Morrison, Adam Perer, Afrooz Zandifar, Shyam Visweswaran, Motahhare Eslami, Kayhan Batmanghelich
    <br><em>IEEE Transactions on Medical Imaging</em> (IEEE T-MI)
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/Research/MedSyn.jpg" alt="MedSyn Project">
          </div>
        </div>
        This work introduces an innovative methodology for producing high-quality 3D lung CT images guided by textual information. Specifically, we introduce a hierarchical scheme that uses a modified UNet architecture. We start by synthesizing low-resolution images conditioned on the text, serving as a foundation for subsequent generators for complete volumetric data. To ensure the anatomical plausibility of the generated samples, we provide further guidance by generating vascular, airway, and lobular segmentation masks in conjunction with the CT images.
      </ul>
    </details>
  </li>
</ol>
  
<hr>

<h3 id="Computer Vision"><font color="black">Topic 2 - </font><font color="#DE3163">Computer Vision</font></h3>
<ol>
  <li>
    <div style="float:left;">
      <font color="#00066f">No-reference Image Quality Assessment via Non-local Dependency Modeling</font>
    </div><br>
    <strong>Shuyue Jia</strong>, Dingquan Li, Shiqi Wang
    <div style="float:right;">
      <a href="./files/MMSP/MMSP22_Poster.pdf"><i class="fa fa-file-powerpoint-o"></i> Poster</a>
      <a href="./files/Presentation/A_Summary_Three_Projects.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/SuperBruceJia/NLNet-IQA"><i class="fa fa-github"></i> Codes</a>
      <a href="https://huggingface.co/shuyuej/NLNet">&#129303;&#32;HuggingFace</a>
      <a href="https://ieeexplore.ieee.org/abstract/document/9950035"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div>
    <br><em>IEEE 24th International Workshop on Multimedia Signal Processing</em> (IEEE MMSP)
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        A no-reference image quality assessment method based on non-local features learned by a graph neural network (GNN). The proposed quality assessment framework is rooted in the view that the human visual system perceives image quality with long-dependency constructed among different regions, inspiring us to explore the non-local interactions in quality prediction.
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Research/Picture5.jpg" alt="NL-Net">
        </div>
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">Learning From Mixed Datasets: A Monotonic Image Quality Assessment Model</font>
    </div>
    <div style="float:right;">
      <a href="https://github.com/SuperBruceJia/MonotonicIQA"><i class="fa fa-github"></i> Codes</a>
      <a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/ell2.12698"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    Zhaopeng Feng, Keyang Zhang, <strong>Shuyue Jia</strong>, Baoliang Chen, Shiqi Wang
    <br><em>IET Electronics Letters</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/Research/MonotonicIQA.png" alt="MonotonicIQA Project">
          </div>
        </div>
        We propose a monotonic neural network for Image Quality Assessment (IQA) model learning with different datasets combined. In particular, our model consists of a dataset-shared quality regressor and several dataset-specific quality transformers. The quality regressor aims to obtain the perceptual qualities of each dataset while each quality transformer maps the perceptual qualities to the corresponding dataset annotations with their monotonicity maintained.
      </ul>
    </details>
  </li>

</ol>

<hr>

<h3 id="Neuroscience"><font color="black">Topic 3 - </font><font color="#DE3163">Neuroscience</font></h3>
<ol>
  <li>
    <div style="float:left;">
      <font color="#00066f">
        GCNs-Net: A Graph Convolutional Neural Network Approach for Decoding Time-resolved EEG Motor Imagery Signals
      </font>
    </div>
    <strong>Shuyue Jia</strong>, Yimin Hou, Xiangmin Lun, Yan Shi, Yang Li, Rui Zeng, Jinglei Lv
    <br>
    
    <div style="float:left;">
      <em>IEEE Transactions on Neural Networks and Learning Systems</em> (IEEE T-NNLS)
    </div>
    <div style="float:right;">
      <a href="./files/EEG/GCNs-Net-Presentation.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/SuperBruceJia/EEG-DL"><i class="fa fa-github"></i> Codes</a>
      <a href="https://ieeexplore.ieee.org/document/9889159"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        Traditional works classify EEG signals without considering the topological relationship among electrodes. Thus, a graph convolutional neural network is presented while cooperating with the functional topological relationship of electrodes.
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/Research/Picture2.png" alt="Project2">
          </div>
        </div>
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor Imagery Recognition</font>
    </div><br>
    Yimin Hou, <strong>Shuyue Jia (Corresponding Author)</strong>, Xiangmin Lun, Shu Zhang, Jinglei Lv
    <div style="float:right;">
      <a href="./files/EEG/BiLSTM-GCN-Presentation.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/SuperBruceJia/EEG-DL"><i class="fa fa-github"></i> Codes</a>
      <a href="https://www.frontiersin.org/article/10.3389/fbioe.2021.706229"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div>
    <br><em>Frontiers in Bioengineering and Biotechnology</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        This paper presents a novel deep learning approach designed toward both remarkably accurate and responsive motor imagery (MI) recognition based on scalp EEG. Bidirectional long short-term memory (BiLSTM) with the attention mechanism is employed, and the graph convolutional neural network (GCN) promotes the decoding performance by cooperating with the topological structure of features.
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="./imgs/Research/Picture3.jpeg" alt="Project3.1">
            <img width="99%device-width" src="./imgs/Research/Picture4.jpeg" alt="Project3.2">
          </div>
        </div>
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f"> 
        A Novel Approach of Decoding EEG Four-Class Motor Imagery Tasks via Scout ESI and CNN
      </font>
    </div>
    <div style="float:right;">
      <a href="https://github.com/SuperBruceJia/EEG-Motor-Imagery-Classification-CNNs-TensorFlow"><i class="fab fa fa-github"></i> Codes</a>
      <a href="https://iopscience.iop.org/article/10.1088/1741-2552/ab4af6/meta"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    Yimin Hou, Lu Zhou, <strong>Shuyue Jia</strong>, Xiangmin Lun
    <br><em>Journal of Neural Engineering</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        We presented a novel approach that could potentially improve the current stroke rehabilitation strategies by implementing a deep learning approach for an Electroencephalogram (EEG) based on MI Brain-Computer Interface System.
        <ul>
          <li>
            Constructed 6 convolutional layers, 2 max-pooling layers, and 3 FC layers CNNs for four-class motor imagery classification through TensorFlow, with 50% dropout (spatial dropout after every Conv layer and regular dropout for FC layers) &ndash; 11.44% accuracy improvement, batch normalization (BN) &ndash; 10.15% improvement, and Short-cut Connection &ndash; 1.76% improvement to prevent overfitting, and achieved SOTA results: 94.50% accuracy on scout R5, 94.54% at subject level, and 96% for left fist prediction.
          </li>
          <li>
            Took charge of DNNs design, including methods comparisons, such as MLPs, CNNs, RNNs, and LSTMs, classification results calculations, and programming. 10 and 14 subjects&rsquo; data were utilized (19,320 and 27,048 samples in the experiments)
          </li>
          <li>
            Benchmark Dataset: <a href="https://archive.physionet.org/pn4/eegmmidb/"> EEG Motor Movement/Imagery Dataset</a>.
          </li>
        </ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="./imgs/Research/Picture1-1.png" alt="EEG-CNN-1">
            <img width="50%device-width" src="./imgs/Research/Picture1-2.png" alt="EEG-CNN-2">
          </div>
        </div>
      </ul>
    </details>
  </li>
</ol>

<hr>

<h3 id="Intelligent Technologies"><font color="black">Topic 4 - </font><font color="#DE3163">Intelligent Technologies</font></h3>
<ol>
  <li>
    <div style="float:left;">
      <font color="#00066f">PMU Measurements based Short-term Voltage Stability Assessment of Power Systems via Deep Transfer Learning</font>
    </div><br>
    Yang Li, Shitu Zhang, Yuanzheng Li, Jiting Cao, <strong>Shuyue Jia</strong>
    <div style="float:right;">
      <a href="https://ieeexplore.ieee.org/abstract/document/10251073"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div>
    <br><em>IEEE Transactions on Instrumentation and Measurement</em> (IEEE T-IM)
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">Improving Performance: A Collaborative Strategy for the Multi-data Fusion of Electronic Nose and Hyperspectral to Track the Quality Difference of Rice</font>
    </div><br>
    Yan Shi, Hangcheng Yuan, Chenao Xiong, Qi Zhang, <strong>Shuyue Jia</strong>, Jingjing Liu, Hong Men
    <div style="float:right;">
      <a href="https://www.sciencedirect.com/science/article/pii/S0925400521001143"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div>
    <br><em>Sensors and Actuators B: Chemical</em>
  </li>
</ol>

<h2 id="Teaching Experience">Teaching Experience</h2>
<ol>
  <li><strong>Teaching Assistant</strong>, Spring 2025 Semester<br>
    <a href="https://www.bu.edu/academics/eng/courses/eng-ec-327/" style="color:black;">ENG EC 327: Introduction to Software Engineering</a>, Boston University<br>
  </li>
</ol>

<h2 id="Academic Services">Academic Services</h2>
<ol>
  <li><strong>Journal Reviewer</strong> of<br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" style="color:black;">IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE T-PAMI)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046" style="color:black;">IEEE Transactions on Multimedia (IEEE T-MM)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" style="color:black;">IEEE Transactions on Circuits and Systems for Video Technology (IEEE T-CSVT)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" style="color:black;">IEEE Transactions on Neural Networks and Learning Systems (IEEE T-NNLS)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42" style="color:black;">IEEE Transactions on Medical Imaging (IEEE T-MI)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10" style="color:black;">IEEE Transactions on Biomedical Engineering (IEEE T-BME)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9424" style="color:black;">IEEE Transactions on Industrial Informatics (IEEE T-II)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221037" style="color:black;">IEEE Transactions on Human-Machine Systems (IEEE T-HMS)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5165369" style="color:black;">IEEE Transactions on Affective Computing (IEEE T-AFFC)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274989" style="color:black;">IEEE Transactions on Cognitive and Developmental Systems (IEEE T-CDS)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7433297" style="color:black;">IEEE Transactions on Emerging Topics in Computational Intelligence (IEEE T-ETCI)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221020" style="color:black;">IEEE Journal of Biomedical and Health Informatics (IEEE JBHI)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4200690" style="color:black;">IEEE Journal of Selected Topics in Signal Processing (IEEE J-STSP)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8782706" style="color:black;">IEEE Open Journal of the Industrial Electronics Society (IEEE OJIES)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8782664" style="color:black;">IEEE Open Journal of the Computer Society (IEEE OJCS)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=93" style="color:black;">IEEE MultiMedia (MM)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7361" style="color:black;">IEEE Sensors Journal</a><br>
    <a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding" style="color:black;">Computer Vision and Image Understanding (CVIU)</a><br>
    <a href="https://ietresearch.onlinelibrary.wiley.com/journal/17519667" style="color:black;">IET Image Processing</a><br>  
    <a href="https://ietresearch.onlinelibrary.wiley.com/journal/17518830" style="color:black;">IET Science, Measurement & Technology</a><br>  
    <a href="https://www.jmir.org" style="color:black;">Journal of Medical Internet Research (JMIR)</a>
  </li>
  <li><strong>Conference Reviewer</strong> of<br>
    <a href="https://iclr.cc" style="color:black;">The International Conference on Learning Representations (ICLR) 2025, 2026</a>
  </li>
  <li><strong>Committee Member</strong> of<br>
    <ul>
      <li><a href="https://ieeeboston.org" style="color:black;">Local Conference Committee, IEEE Boston Section</a></li>
      <li><a href="https://ieee-icad.org/icad-committee/" style="color:black;">Speaker Program Chair (Keynotes & Panel Sessions) and Steering Committee, 2026 IEEE International Conference on AI and Data Analytics (ICAD)</a></li>
      <li><a href="https://ieee-icad.org/icad-committee/" style="color:black;">Speaker Program Chair (Keynotes & Panel Sessions) and Steering Committee, 2025 IEEE International Conference on AI and Data Analytics (ICAD)</a></li>
      <li><a href="https://amia.org/education-events/amia-2025-clinical-informatics-conference/spc" style="color:black;">Scientific Program Committee Member, 2025 AMIA Clinical Informatics Conference</a></li>
    </ul>
  </li>
  <li><strong>Member</strong> of <a href="./awards/SigmaXi_2025.pdf" style="color:black;">Sigma Xi</a>, <a href="https://www.ieee.org" style="color:black;">IEEE</a>, <a href="https://www.acm.org" style="color:black;">ACM</a>, <a href="https://www.aclweb.org/portal/what-is-cl" style="color:black;">ACL</a>, and <a href="https://aaai.org" style="color:black;">AAAI</a></li>
</ol>

<h2 id="Selected Awards">Selected Awards</h2>
<ol>
  <li>
    <div style="float:left;">
      <font color="black">
        2025 IEEE Boston Section Arthur Winston Student Award, IEEE Boston Section
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none"><em>"For outstanding contributions to the Boston Section Local Conference Committee"</em></summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./awards/IEEE-Boston-Section-Award.jpg" alt="IEEE-2025">
        </div>
        <div style="text-align:center">
          <img width="99%device-width" src="./awards/IEEE_Award_2025_1.jpg" alt="IEEE-2025">
        </div>
        <div style="text-align:center">
          <img width="99%device-width" src="./awards/IEEE_Award_2025_2.jpg" alt="IEEE-2025">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        Wiley Top Cited Article 2023-2024, Wiley Top Cited Papers, Wiley
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none"><em>"Our work has been recognized as a top cited article in Electronics Letters"</em></summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./awards/Wiley-Top-Cited-Article-Certificate-2023-2024.jpg" alt="IEEE-2025">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        CityU Top 5 Runner, City University of Hong Kong
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">Athlete</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Marathon/cityu_marathon_2021.jpg" alt="marathon-2021">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        Outstanding Athlete, Northeast Electric Power University
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">Athlete</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Marathon/EliteAthlete.jpg" alt="Elite Athlete">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        3000-meter Steeplechase, The 45th Northeast Electric Power University Games
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">The 7th Place in college</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Marathon/Steeplechase.jpg" alt="Steeplechase">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2015 National High School Math League, China
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">Second Prize</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Competition/Math.jpg" alt="Math">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        The 32nd Chinese Physics Olympiad (CPhO), China
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">Third Prize</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Competition/Physics.jpg" alt="Physics">
        </div>
      </div>
    </details>
  </li>  
</ol>

<h2 id="Academic Services">International Marathon Athlete Activities</h2>
<ol>
  <li>
    <div style="float:left;">
      <font color="black">
        2025 Boston Half Marathon
      </font>
    </div>
  </li>
  
  <br>

  <li>
    <div style="float:left;">
      <font color="black">
        2025 The 129th Boston Marathon, Abbott World Marathon Majors
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2024 Boston Half Marathon
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2023 Standard Chartered Hong Kong Marathon, Full Marathon
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2021 Hangzhou International Marathon, Half Marathon
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2021 Standard Chartered Hong Kong Marathon, Half Marathon
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2018 National Marathon Championships (Jilin City Station), Full Marathon
      </font>
    </div>
    
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2017 National Marathon Championships (Jilin City Station), Half Marathon
      </font>
    </div>
    
  </li>
</ol>

<br>
<br>

</div>
</div>

<hr>
<br>
</body>

<footer style="text-align:center">
  <a style="font-family: 'STKaiti'; font-size: 20px; display: block; margin-bottom: 2px; color: black;text-decoration: none;">æ–‡ç« åƒå¤äº‹ï¼Œå¾—å¤±å¯¸å¿ƒçŸ¥ã€‚</a>
  ğ™ğ™ğ™£ğ™™ ğ™©ğ™ğ™š ğ™˜ğ™¤ğ™ªğ™§ğ™–ğ™œğ™š ğ™©ğ™¤ ğ™—ğ™šğ™®ğ™¤ğ™£ğ™™ ğ™—ğ™¤ğ™ªğ™£ğ™™ğ™–ğ™§ğ™ğ™šğ™¨!<br>
  <br>
  <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">
    <img alt="Creative Commons License" style="border-width:0"
      src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" />
  </a>
  <span><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></span>
</footer>

<br>
<br>
<br>
<br>
</html>