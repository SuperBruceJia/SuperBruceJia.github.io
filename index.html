<!DOCTYPE html>
<html lang="en">

<head>
  <title>Shuyue Jia (Bruce Jia)</title>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Shuyue Jia, 贾舒越, Bruce Jia, Jia Shuyue, Bruce Shuyue Jia, Bruce S. Jia, Shuyue, shuyue, shuyue jia, jia shuyue, jiashuyue, shuyuejia, shuyuej, 舒越, 舒越贾, 贾 舒越, 舒越 贾, shuyu jia, jia shuyu, 舒, 越, 贾, 贾舒, 贾越, 贾 舒 越, SHUYUE JIA, SHUYUE, JIA SHUYUE, JIASHUYUE, SHU, YUE, JIA, JIASHU, JIA SHU, JIA SHU YUE, Bruce Jia, Bruce, Bruce Shuyue Jia, Bruce Shuyue, Bruce Shuyue JIA, SHUYUE JIA, SHUYUE, JIA SHUYUE, JIASHUYUE, SHU, YUE, JIA, JIASHU, JIA SHU, JIA SHU YUE">
    <meta name="author" content="Shuyue Jia">
    <meta name="description" content="This is Shuyue Jia's Homepage! Reference: Shuyue Jia, 贾舒越, Bruce Jia, Jia Shuyue, Bruce Shuyue Jia, Bruce S. Jia, Shuyue, shuyue, shuyue jia, jia shuyue, jiashuyue, shuyuejia, shuyuej, 舒越, 舒越贾, 贾 舒越, 舒越 贾, shuyu jia, jia shuyu, 舒, 越, 贾, 贾舒, 贾越, 贾 舒 越, SHUYUE JIA, SHUYUE, JIA SHUYUE, JIASHUYUE, SHU, YUE, JIA, JIASHU, JIA SHU, JIA SHU YUE, Bruce Jia, Bruce, Bruce Shuyue Jia, Bruce Shuyue, Bruce Shuyue JIA, SHUYUE JIA, SHUYUE, JIA SHUYUE, JIASHUYUE, SHU, YUE, JIA, JIASHU, JIA SHU, JIA SHU YUE">
    <meta name="google-site-verification" content="UZ7grdeibwJYn35EsW-Yp0Ky7_vuMG45bHI1kki15H0"/>
    <meta name="baidu-site-verification" content="WJpwHTPWUq"/>
    
    <style>
      div{
        line-height:28px;
      }
      span.highlight {
        background-color: #ffffd0;
      }
    </style>
    
    <link rel="shortcut icon" href="bu.ico">
    <link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700" rel="stylesheet" type="text/css">
    <link rel="alternate" type="application/rss+xml" title="Shuyue Jia RSS" href="/feed.xml" />
    <link rel="stylesheet" href="/css/main.css">
    <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="stylesheet" href="css/css/academicons.min.css"/>
    <link href="css/main.css" rel="stylesheet" type="text/css">
</head>

<style type="text/css">
    p{
      text-align:justify;
    }
    
    p i{
      display:inline-block;
      width:100%;
    }
    
    ul{
      text-align:justify;
    }
    
    ul i{
      display:inline-block;
      width:100%;
    }
</style>

<style>
  .arrow-list {
    list-style-type: none; /* Removes default bullets */
    padding-left: 0;
    margin: 0; /* Remove extra margins around the list */
  }
  
  .arrow-list li {
    font-size: 24px; /* Set font size */
    line-height: 1.0; /* Shorter line spacing */
  }
  
  .arrow-list li::before {
    content: '→'; /* Unicode rightward arrow */
    margin-right: 8px; /* Add space between arrow and text */
  }
</style>

<body>
<div class="wrapper">
  <div class="navbar container">
    <a id="author-name" class="alignable pull-left"
      style="text-decoration:none; display: flex; align-items: center; gap: 0.05em;">
      Shuyue Jia (<img class="chinese-name-img" src="./imgs/Profile/chinese_name.png"
        alt="贾舒越" style="width: 170px; height: auto; vertical-align: middle;">)
    </a>
    <a id="author-name" class="alignable pull-right"
      style="width: 260px; text-decoration:none; display: flex; align-items: center; transform: translateY(15px);"
      href="https://brucejia.myportfolio.com">Photography</a>
  </div>
<div style="clear:both"></div>

<hr>

<div class="container content">

<h2 id="about-me">Bio</h2>
<div class="profile-container">
  <img class="profile-picture" src="./imgs/Profile/Big-Buddha-Shuyue-Profile.jpg"/>
  <p class="image-description">𝒯𝒾𝒶𝓃 𝒯𝒶𝓃 𝐵𝓊𝒹𝒹𝒽𝒶, 𝐻𝑜𝓃𝑔 𝒦𝑜𝓃𝑔</p>
</div>

<p>
  I am a Ph.D. student in Computer Engineering at <a href="https://www.bu.edu">Boston University</a>, under the supervision of <a href="https://vkola-lab.github.io/team" style="color:black;">Prof. Vijaya Kolachalama</a>. Prior to BU, I received my M.Phil. degree (2023) in Computer Science from <a href="https://www.cityu.edu.hk">City University of Hong Kong</a>, supervised by <a href="https://www.cs.cityu.edu.hk/~shiqwang" style="color:black;">Prof. Shiqi Wang</a>, and B.Eng. degree (2020) in Intelligence Science and Technology from <a href="https://www.neepu.edu.cn">Northeast Electric Power University</a>, supervised by <a href="https://ieeexplore.ieee.org/author/37290052900" style="color:black;">Prof. Yimin Hou</a> and <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/jinglei-lv.html" style="color:black;">Prof. Jinglei Lv</a>. In 2017, I attended a summer school at <a href="https://uci.edu">University of California, Irvine</a>.
  
  <br>
  <br>
  
  𝗥𝗲𝘀𝗲𝗮𝗿𝗰𝗵 𝗜𝗻𝘁𝗲𝗿𝗲𝘀𝘁𝘀:<br>
  My Ph.D. research focuses on <b>grounding AI systems</b> as a pathway toward Artificial General Intelligence (AGI), with an emphasis on developing safe, reliable, and extensible foundations. Specifically, my work focuses on the following dimensions:
  <li>𝗠𝗲𝗮𝘀𝘂𝗿𝗲𝗺𝗲𝗻𝘁
    <ul class="arrow-list">
      <li>Quality Estimation of Natural Language Generation (NLG)</li>
      <li>Quality Assessment of Natural Image [<a href="https://ieeexplore.ieee.org/abstract/document/9950035">1</a>, <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ell2.12698">2</a>]</li>
    </ul>
  </li>

  <li>𝗠𝗼𝗱𝗲𝗹𝗶𝗻𝗴
    <ul class="arrow-list">
      <li>Multimodal Foundation Model (FM)</li>
      <li>LLM-based Agent, Vision-language Agent (VLA), and Multi-agent System (MAS) [<a href="https://www.medrxiv.org/content/10.1101/2025.08.06.25333160v1">1</a>]</li>
      <li>Large Vision-language Model (VLM)</li>
      <li>Diffusion Model [<a href="https://ieeexplore.ieee.org/document/10566053">1</a>]</li>
      <li>Graph CNN (GCN) [<a href="https://ieeexplore.ieee.org/document/9889159">1</a>]</li>
      <li>Attention-based BiLSTM-GCN [<a href="https://www.frontiersin.org/journals/bioengineering-and-biotechnology/articles/10.3389/fbioe.2021.706229/full">1</a>]</li>
      <li>CNN [<a href="https://iopscience.iop.org/article/10.1088/1741-2552/ab4af6">1</a>]</li>
    </ul>
  </li>

  <li>𝗢𝗽𝘁𝗶𝗺𝗶𝘇𝗮𝘁𝗶𝗼𝗻
    <ul class="arrow-list">
      <li>Continual Pre-training of LLM [<a href="https://www.medrxiv.org/content/10.1101/2024.07.11.24310304v1">1</a>, <a href="https://www.nature.com/articles/s44385-025-00022-0">2</a>]</li>
      <li>Retrieval-augmented Generation (RAG) [<a href="https://www.nature.com/articles/s44385-025-00022-0">1</a>]</li>
    </ul>
  </li>

  <li>𝗧𝗼𝗼𝗹𝗸𝗶𝘁 & 𝗟𝗶𝗯𝗿𝗮𝗿𝘆 (𝗢𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗣𝗿𝗼𝗷𝗲𝗰𝘁)
    <ul class="arrow-list">
      <li><a href="https://github.com/vkola-lab/PodGPT">PodGPT</a>: An audio-augmented LLM for research and education
        <iframe
          src="https://img.shields.io/github/stars/vkola-lab/PodGPT?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe>
        <iframe
          src="https://img.shields.io/github/forks/vkola-lab/PodGPT?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe> 
      </li>
      <li><a href="https://github.com/vkola-lab/PodGPT/tree/main/rag_pipeline">PodRAG</a>: PodGPT with a retrieval, reranking, and generation pipeline
        <iframe
          src="https://img.shields.io/github/stars/vkola-lab/PodGPT?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe>
        <iframe
          src="https://img.shields.io/github/forks/vkola-lab/PodGPT?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe>
      </li>
      <li><a href="https://github.com/SuperBruceJia/EEG-DL">EEG-DL</a>: A deep learning library for EEG signals classification
        <iframe
          src="https://img.shields.io/github/stars/SuperBruceJia/EEG-DL?style=social" 
          frameborder="0" scrolling="0" width="90px" height="20px">
        </iframe>
        <iframe
          src="https://img.shields.io/github/forks/SuperBruceJia/EEG-DL?style=social" 
          frameborder="0" scrolling="0" width="100px" height="20px">
        </iframe>
      </li>
      <li><a href="https://github.com/SuperBruceJia/promptcraft">Prompt Perturbation</a>: A perturbation toolkit for prompt augmentation
        <iframe
          src="https://img.shields.io/github/stars/SuperBruceJia/promptcraft?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe>
        <iframe
          src="https://img.shields.io/github/forks/SuperBruceJia/promptcraft?style=social" 
          frameborder="0" scrolling="0" width="82px" height="20px">
        </iframe> 
      </li>
    </ul>
  </li>
  
  <li>𝗠𝗮𝗶𝗻𝘁𝗮𝗶𝗻𝗲𝗱 𝗚𝗶𝘁𝗛𝘂𝗯 𝗥𝗲𝗽𝗼 𝗳𝗼𝗿 𝗟𝗶𝘁𝗲𝗿𝗮𝘁𝘂𝗿𝗲 𝗦𝘂𝗿𝘃𝗲𝘆<br>
    <ul class="arrow-list">
      Welcome to contribute and work together!
      <li><a href="https://github.com/SuperBruceJia/Awesome-Large-Vision-Language-Model">Awesome Large Vision-Language Model (VLM)</a></li>
      <li><a href="https://github.com/SuperBruceJia/Awesome-Mixture-of-Experts">Awesome Mixture of Experts (MoE)</a></li>
      <li><a href="https://github.com/SuperBruceJia/Awesome-LLM-Self-Consistency">Awesome LLM Self-Consistency</a></li>
      <li><a href="https://github.com/SuperBruceJia/Awesome-Text-Generation-Evaluation">Awesome Text/Natural Language Generation (NLG) Evaluation</a></li>
      <li><a href="https://github.com/SuperBruceJia/Awesome-Semantic-Textual-Similarity">Awesome Semantic Textual Similarity (STS)</a></li>
    </ul>
  </li>

  <br>
  
  <details>
    <summary style="outline:none">Research Focus Illustration</summary>
    <div style="text-align:center">
      <img width="99%device-width" src="./imgs/Multimodal/Multimodal-1.jpg" />
      <img width="99%device-width" src="./imgs/Multimodal/Multimodal-2.jpg" />
      <img width="99%device-width" src="./imgs/Multimodal/Multimodal-3.jpg" />
    </div>
  </details>
  
  <details>
      <summary style="outline:none">Research Presentations and Resources</summary>
      
      <br><a style="color:crimson;">𝐖𝐡𝐚𝐭 𝐢𝐬 𝐭𝐡𝐞 𝐦𝐨𝐬𝐭 𝐬𝐮𝐫𝐩𝐫𝐢𝐬𝐢𝐧𝐠 𝐟𝐢𝐧𝐝𝐢𝐧𝐠 𝐢𝐧 𝐲𝐨𝐮𝐫 𝐫𝐞𝐬𝐞𝐚𝐫𝐜𝐡?</a>
      
      <p><font color="#9933CC">𝐏𝐫𝐞𝐬𝐞𝐧𝐭𝐚𝐭𝐢𝐨𝐧</font> - Artificial Intelligence (AI)</p>
        <li><a href="./files/Presentation/Current-Artificial-Intelligence-2023-ShuyueJIA.pptx"> <i class="fa fa-file-powerpoint-o"></i> AI In the 2020s And Beyond</a></li>
      
      <p><font color="#9933CC">𝐏𝐫𝐞𝐬𝐞𝐧𝐭𝐚𝐭𝐢𝐨𝐧</font> - Graph Neural Network (GNN)</p>
        <li><a href="./files/EEG/GCNs-Net-Summary.pdf"> <i class="fab fa fa-file-pdf-o"></i> Graph Convolutional Neural Networks</a></li>
        <li><a href="./files/EEG/BiLSTM-GCN-Summary.pdf"> <i class="fab fa fa-file-pdf-o"></i> Attention-based BiLSTM-GCN</a></li>
        <li><a href="./files/EEG/Dynamic-GCN-Survey.pdf"> <i class="fab fa fa-file-pdf-o" style="text-decoration:none"></i> Dynamic Graph Convolutional Neural Networks</a></li>
      
      <p><font color="#9933CC">𝐏𝐫𝐞𝐬𝐞𝐧𝐭𝐚𝐭𝐢𝐨𝐧</font> - Natural Language Processing (NLP)</p>
        <li><a href="./files/Presentation/Foundation-Models-Sequential-Decision-Making.pdf"> <i class="fab fa fa-file-pdf-o"></i> Foundation Models for Sequential Decision Making</a></li>
        <li><a href="./files/Presentation/Factual-Associations-LLMs.pdf"> <i class="fab fa fa-file-pdf-o"></i> Factual Associations in LLMs</a></li>
        <li><a href="./files/Graph-Matching-Paper-Survey.pdf"> <i class="fab fa fa-file-pdf-o"></i> Graph Matching</a></li>
        <li><a href="./files/NMT-Subword-Unites.pdf"> <i class="fab fa fa-file-pdf-o"></i> Sub-word BPE Algorithm for NMT</a></li>
        <li><a href="./files/Concept-Matching.pdf"> <i class="fab fa fa-file-pdf-o"></i> Concept Matching for Medical Terms</a></li>
      
      <p><font color="#9933CC">𝐏𝐫𝐞𝐬𝐞𝐧𝐭𝐚𝐭𝐢𝐨𝐧</font> - Computer Vision (CV)</p>
        <li><a href="./files/Presentation/IQA-Perceptual-Optimization.pdf"> <i class="fab fa fa-file-pdf-o"></i> Image Quality Assessment and Perceptual Optimization</a></li>
        <li><a href="./files/Model-Compression-Acceleration.pdf"> <i class="fab fa fa-file-pdf-o"></i> Deep Learning Models Compression and Acceleration</a></li>
        <li><a href="./files/Spatial-Sparse-CNNs.pdf"> <i class="fab fa fa-file-pdf-o"></i> 3D Human Pose Estimation and Human Body Reconstruction</a></li>
        <li><a href="./files/YOLO.pdf"> <i class="fab fa fa-file-pdf-o"></i> YOLO Object Detection</a></li>
      
      <p><font color="#9933CC">𝐏𝐫𝐞𝐬𝐞𝐧𝐭𝐚𝐭𝐢𝐨𝐧</font> - Tutorials and Useful Coding Scripts</p>
        <li><a href="./files/Server.pdf"> <i class="fab fa fa-file-pdf-o"></i> Usage of Cloud Server and Setting-up</a></li>
        <li><a href="./files/Python.pdf"> <i class="fab fa fa-file-pdf-o"></i> Python Environment Setting-up</a></li>
        <li><a href="./files/TensorFlow.pdf"> <i class="fab fa fa-file-pdf-o"></i> TensorFlow for Deep Learning</a></li>
        <li><a href="./files/Crypto-currency-Return-and-Price-Prediction-with-Machine-Learning.pdf"><i class="fab fa fa-file-pdf-o"></i> Crypto Currency Return and Price Prediction with Machine Learning</a></li>
        <li><a href="./files/PySpark-and-Horovod.pdf"> <i class="fab fa fa-file-pdf-o"></i> Big Data Parallel Processing by PySpark and Horovod Distributed Deep Learning</a></li>
        <li><a href="https://github.com/SuperBruceJia/Sci-Hub-Paper-Download-shell"> <i class="fab fa-github"></i> Download Papers from Sci-Hub via Unix Shell</a></li>
        <li><a href="https://github.com/SuperBruceJia/Google-Scholar-Citations-Download"> <i class="fab fa-github"></i> Retrieve and Download Google Scholar Citation Papers</a></li>
        <li><a href="https://github.com/SuperBruceJia/dynamic-web-crawlering-python"> <i class="fab fa-github"></i> Dynamic (Ajax) Web Crawler in Python</a></li>
        <li><a href="https://github.com/SuperBruceJia/pytorch-flask-deploy-webapp"> <i class="fab fa-github"></i> Deploy Machine Learning and Deep Learning Models with Flask and Docker as Web Applications</a></li>
        <li><a href="https://github.com/SuperBruceJia/CityU-MPhil-Thesis"> <i class="fab fa-github"></i> M.Phil. Thesis (LaTeX), City University of Hong Kong</a></li>

      <p><font color="#9933CC">𝐄𝐱𝐜𝐞𝐥𝐥𝐞𝐧𝐭 𝐏𝐫𝐞𝐬𝐞𝐧𝐭𝐚𝐭𝐢𝐨𝐧</font></p>
        <li><a href="./resources/PowerPoint/On%20the%20inductive%20bias%20of%20language%20modeling.pptx"> <i class="fa fa-file-powerpoint-o" style="font-size:24px;color:red"></i> On the Inductive Bias of Language Modeling</a> | <a href="https://youtu.be/PNTbvoweqBk?t=3046"> <i class="fa fa-youtube-play" style="font-size:24px;color:red"></i> Oral Presentation (Tatsunori B. Hashimoto)</a></li>
      
      <p><font color="#9933CC">𝐓𝐞𝐜𝐡𝐧𝐢𝐜𝐚𝐥 𝐁𝐨𝐨𝐤𝐬</font></p>
        <li><a href="./books/Foundation-Models-for-Natural-Language-Processing.pdf"> <i class="fab fa fa-book"></i> Foundation Models for Natural Language Processing: Pre-trained Language Models Integrating Media (Gerhard Paaß and Sven Giesselbach)</a></li>
        <li><a href="./books/The-Path-to-Artificial-General-Intelligence.pdf"> <i class="fab fa fa-book"></i> The Path to Artificial General Intelligence: Insights from Adversarial LLM Dialogue (Edward Y. Chang)</a></li>
        <li><a href="./books/Pattern%20Recognition%20and%20Machine%20Learning.pdf"> <i class="fab fa fa-book"></i> Pattern Recognition and Machine Learning (Christopher Bishop)</a></li>
        <li><a href="./books/Reinforcement%20Learning-%20An%20Introduction.pdf"> <i class="fab fa fa-book"></i> Reinforcement Learning: An Introduction (Richard S. Sutton and Andrew G. Barto)</a></li>
        <li><a href="./books/Digital%20Image%20Processing.pdf"> <i class="fab fa fa-book"></i> Digital Image Processing (Rafael C. Gonzalez and Richard E. Woods)</a></li>
        <li><a href="./books/Multiple%20View%20Geometry%20in%20Computer%20Vision.pdf"> <i class="fab fa fa-book"></i> Multiple View Geometry in Computer Vision (Richard Hartley and Andrew Zisserman)</a></li>
        <li><a href="./books/Graph%20Representation%20Learning.pdf"> <i class="fab fa fa-book"></i> Graph Representation Learning (William L. Hamilton)</a></li>
        <li><a href="./books/Computer%20Systems-%20A%20Programmer's%20Perspective.pdf"> <i class="fab fa fa-book"></i> Computer Systems: A Programmer's Perspective (Randal E. Bryant and David R. O'Hallaron)</a></li>
        <li><a href="./books/Computer%20Organization%20and%20Design-%20The%20Hardware%3ASoftware%20Interface.pdf"> <i class="fab fa fa-book"></i> Computer Organization and Design: The Hardware/Software Interface (David A. Patterson and John L. Hennessy)</a></li>
      
      <p><font color="#9933CC">𝐀𝐜𝐚𝐝𝐞𝐦𝐢𝐜 𝐑𝐞𝐬𝐨𝐮𝐫𝐜𝐞𝐬</font></p>
        <li><a href="./resources/How-to-be-a-good-Meta-Reviewer.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to be a Good Meta-reviewer? by ICML 2022 Program Chairs</a></li>
        <li><a href="./resources/How_Junior_Faculty_Can_Secure_Industry_Funding.pdf"> <i class="fab fa fa-file-pdf-o"></i> How Junior Faculty Can Secure Industry Funding? by Prof. Zhou Yu</a></li>
        <li><a href="./resources/Writing-and-Technical-Presentations.pdf"> <i class="fab fa fa-file-pdf-o"></i> Writing and Technical Presentations by Prof. Ayse Coskun</a></li>
        <li><a href="./resources/English-Writing.pdf"> <i class="fab fa fa-file-pdf-o"></i> The Most Common Habits from more than 200 English Papers written by Graduate Chinese Engineering Students by Felicia Brittman</a></li>
        <li><a href="./resources/How_to_Write_Good_Research_Articles_Xiaohua_JIA.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to Write Good Research Articles by Prof. Xiaohua Jia</a></li>
        <li><a href="./resources/业余做研究的经验-田渊栋.pdf"> <i class="fab fa fa-file-pdf-o"></i> 业余做研究的经验 by Dr. Yuandong Tian</a></li>
        <li><a href="./resources/How_to_do_Research.ppt"> <i class="fa fa-file-powerpoint-o" style="font-size:24px;color:red"></i> What is Research and How to do it? by Prof. Yi Ma</a></li>
        <li><a href="./resources/CVPR_Submission.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to Write a Good CVPR Submission? by Prof. Bill Freeman</a></li>
        <li><a href="./resources/Tips-on-Writing-Papers-with-Mathematical-Content.pdf"> <i class="fab fa fa-file-pdf-o"></i> Tips on Writing Papers with Mathematical Content by Prof. John N. Tsitsiklis</a></li>
        <li><a href="./resources/How-to-be-a-good-Reviewer.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to Be a Responsible Reviewer by Prof. Jiebo Luo</a></li>
        <li><a href="./resources/How_to_Use_IEEEtran_LaTeX_Class.pdf"> <i class="fab fa fa-file-pdf-o"></i> How to Use the IEEEtran LaTeX Class</a></li>
        <li><a href="./resources/IEEE-Editing-Mathematics.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Editing Mathematics Guide (Standard)</a></li>
        <li><a href="./resources/IEEE_Math_Typesetting_Guide.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Math Typesetting Guide (LaTeX)</a></li>
        <li><a href="./resources/Formula_Comma_IEEE.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Formula Comma and Period</a></li>
        <li><a href="./resources/IEEE-Reference-Guide-2021.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Reference Guide (2021)</a></li>
        <li><a href="./resources/IEEE_Citation_Examples.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Citation Examples</a></li>
        <li><a href="https://woodward.library.ubc.ca/research-help/journal-abbreviations/"> <i class="fab fa-internet-explorer"></i> Science and Engineering Journal Abbreviations</a></li>
        <li><a href="https://libguides.uwf.edu/c.php?g=848362&p=6073747"> <i class="fab fa-internet-explorer"></i> Standard abbreviations used in the IEEE Reference list</a></li>
        <li><a href="./resources/IEEE-Editorial-Style-Manual-2021.pdf"> <i class="fab fa fa-file-pdf-o"></i> IEEE Editorial Style Manual (2021)</a></li>
        <li><a href="./resources/LATEX-Mathematical-Symbols.pdf"> <i class="fab fa fa-file-pdf-o"></i> LaTeX Mathematical Symbols</a></li>
        <li><a href="./resources/Typesetting-Subtleties-Prof-Vivek-Goyal.pdf"> <i class="fab fa fa-file-pdf-o"></i> Typesetting Subtleties by Prof. Vivek Goyal</a></li>
        <li><a href="./resources/Markdown_Mathematics.pdf"> <i class="fab fa fa-file-pdf-o"></i> Writing Mathematical Formulas in Markdown</a></li>
        <li><a href="./resources/JCR_2021.pdf"> <i class="fab fa fa-file-pdf-o"></i> InCites Journal Citation Reports (2021)</a></li>
        <li><a href="./resources/中国计算机学会CCF推荐国际学术会议和期刊目录_2019.pdf"> <i class="fab fa fa-file-pdf-o"></i> 中国计算机学会推荐国际学术会议和期刊目录 (2019)</a></li>
        <li><a href="./resources/中国科学院SCI分区表_2019.xls"> <i class="fa fa-file-excel-o" style="font-size:24px;color:red"></i> 中国科学院SCI分区表 (2019)</a></li>
        <li><a href="https://earlywarning.fenqubiao.com/#/README"> <i class="fab fa-internet-explorer"></i> 中国科学院国际期刊预警名单</a></li>
        <li><a href="https://github.com/SuperBruceJia/CVPR-LaTeX-Paper-Template"> <i class="fab fa-github"></i> CVPR Paper, Supplementary Materials, and Rebuttal LaTeX Templates</a></li>
        <li><a href="https://github.com/SuperBruceJia/Poster_Template"> <i class="fab fa-github"></i> PowerPoint (PPT) and LaTeX Demos of Academic Posters</a></li>
        <li><a href="./resources/NSF-Guidelines.pdf"> <i class="fab fa fa-file-pdf-o"></i> National Science Foundation (NSF) Proposal & Award Policies & Procedures Guide (PAPPG)</a></li>
        <li><a href="./resources/NSF-Bio.pdf"> <i class="fab fa fa-file-pdf-o"></i> National Science Foundation (NSF) Biographical Sketch</a></li>
        <li><a href="https://new.nsf.gov/policies/pappg/23-1/table-of-contents"> <i class="fab fa-internet-explorer"></i> NSF's Proposal Preparation & Submission Guidelines</a></li>
        <li><a href="./resources/ACL-Fellow-Nomination.pdf"> <i class="fab fa fa-file-pdf-o"></i> ACL Fellow Nomination Form</a></li>
        <li><a href="./resources/CityU-AP.pdf"> <i class="fab fa fa-file-pdf-o"></i> CityU AP Promotion</a></li>
        <li><a href="./resources/RICE-Offer.pdf"> <i class="fab fa fa-file-pdf-o"></i> RICE AP Offer</a></li>
        <li><a href="./resources/MIT-EECS-Courses.pdf"> <i class="fab fa fa-file-pdf-o"></i> MIT EECS Courses</a></li>
        <li><a href="./resources/MIT-EECS-Labs.pdf"> <i class="fab fa fa-file-pdf-o"></i> MIT EECS Organization and Labs (2019)</a></li>
        <li><a href="https://aiindex.stanford.edu/report/"> <i class="fab fa-internet-explorer"></i> Artificial Intelligence Index Report by Stanford</a></li>
        <li><a href="https://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html"> <i class="fab fa-internet-explorer"></i> Defining the Role of Authors and Contributors by International Committee of Medical Journal Editors</a></li>

      <p><font color="#9933CC">𝐑𝐞𝐜𝐨𝐦𝐦𝐞𝐧𝐝𝐞𝐝 𝐑𝐞𝐚𝐝𝐢𝐧𝐠𝐬</font></p>
        <li><a href="./resources/Research_Life.pdf"> <i class="fab fa fa-file-pdf-o"></i> 文章千古事, 得失寸心知 by Prof. Song-Chun Zhu</a></li>
        <li><a href="./resources/Statement_of_Purpose-Kai-Fu-LEE-CMU.pdf"> <i class="fab fa fa-file-pdf-o"></i> Statement of Purpose by Dr. Kai-Fu Lee</a></li>
        <li><a href="./resources/AI-Dartmouth-College-1956.pdf"> <i class="fab fa fa-file-pdf-o"></i> Dartmouth Summer Project on Artificial Intelligence (1956)</a></li>
        <li><a href="./resources/The_Summer_Vision_Project_1966.pdf"> <i class="fab fa fa-file-pdf-o"></i> The Summer Vision Project at MIT (1966)</a></li>
        <li><a href="./resources/Turing-Test-1950.pdf"> <i class="fab fa fa-file-pdf-o"></i> Turing Test (1950) by Alan Mathison Turing</a></li>
        <li><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"> <i class="fab fa-internet-explorer"></i> The Bitter Lesson from 70 years of AI research by Richard Sutton</a></li>
        <li><a href="./resources/Academic-Achievements-Richard.pdf"> <i class="fab fa fa-file-pdf-o"></i> Academic Achievements of Prof. Richard Yi-Da Xu</a></li>
        <li><a href="./resources/Bio-Richard.pdf"> <i class="fab fa fa-file-pdf-o"></i> Brief Bio of Prof. Richard Yi-Da Xu</a></li>
        <li><a href="./resources/Prompt-Engineering.pdf"> <i class="fab fa fa-file-pdf-o"></i> Prompt Template Engineering Experiment by Prof. Edward Chang</a></li>
        <li><a href="./resources/心靈之旅.pdf"> <i class="fab fa fa-file-pdf-o"></i> 心灵之旅 by Prof. Edward Chang</a></li>
        <li><a href="./resources/What-is-life.pdf"> <i class="fab fa fa-file-pdf-o"></i> 生命是什么? by Prof. Edward Chang</a></li>
        <li><a href="./resources/The-PhD-Grind.pdf"> <i class="fab fa fa-file-pdf-o"></i> The Ph.D. Grind by Prof. Philip J. Guo</a></li>
        <li><a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html"> <i class="fab fa-internet-explorer"></i> You and Your Research by Prof. Richard Hamming</a></li>
        <li><a href="https://news.stanford.edu/2005/06/12/youve-got-find-love-jobs-says/"> <i class="fab fa-internet-explorer"></i> You’ve Got to Find What You Love by Steve Jobs</a></li>
        <li><a href="./resources/Computer_Vision_History.pdf"> <i class="fab fa fa-file-pdf-o"></i> A Brief History of Computational Vision by Prof. Song-Chun Zhu</a></li>
        <li><a href="./resources/Artificial_Intelligence.pdf"> <i class="fab fa fa-file-pdf-o"></i> Artificial Intelligence by Prof. Song-Chun Zhu</a></li>
        <li><a href="./resources/上海交通大学学生生存手册.pdf"> <i class="fab fa fa-file-pdf-o"></i> 上海交通大学学生生存手册</a></li>
        <li><a href="https://linshanlee.com/"> <i class="fab fa-internet-explorer"></i> 李琳山教授个人数位典藏</a></li>
        <li><a href="https://www.youtube.com/watch?v=ji5_MqicxSo"> <i class="fa fa-youtube-play" style="font-size:24px;color:red"></i> Randy Pausch Last Lecture: Achieving Your Childhood Dreams</a></li>
  </details>
  
  <details>
    <summary style="outline:none">Teaching Materials</summary>
    <p><font color="#9933CC">ENG EC 327: Introduction to Software Engineering, Boston University (Spring 2025)</font></p>
    <li><a href="./teaching/BU-Spring-2025-ECE327-Introduction-to-Software-Engineering/EC327-Lab-Session-1.pdf"> <i class="fab fa fa-file-pdf-o"></i> Lab Session 1: Linux/Unix Tutorial - Your First Technical Course for Industry Preparation</a></li>
    <li><a href="./teaching/BU-Spring-2025-ECE327-Introduction-to-Software-Engineering/EC327-Lab-Session-2.pdf"> <i class="fab fa fa-file-pdf-o"></i> Lab Session 2: Programming in C/C++: A Hands-on Introduction</a></li>
    <li><a href="./teaching/BU-Spring-2025-ECE327-Introduction-to-Software-Engineering/EC327-Lab-Session-4.pdf"> <i class="fab fa fa-file-pdf-o"></i> Lab Session 4: GitHub: Building Your Coding Profile</a></li>
    <li><a href="./teaching/BU-Spring-2025-ECE327-Introduction-to-Software-Engineering/EC327-Lab-Session-7.pdf"> <i class="fab fa fa-file-pdf-o"></i> Lab Session 7: Unit Testing in Android with GitHub Action</a></li>
  </details>
  
  <details>
    <summary style="outline:none">Contact and Our Team</summary>
    <p><font color="#9933CC">𝗖𝗼𝗻𝘁𝗮𝗰𝘁</font></p>
    Shuyue Jia (Bruce Jia)<br>
    Department of ECE, Boston University<br>
    Add: Kolachalama Lab, 14/F, Center for Computing & Data Sciences,<br>
    Boston University, 665 Commonwealth Ave., Boston, MA 02215<br>
    Tel: +1 (617) 685-1479<br>
    Email: <a href="mailto:brucejia@bu.edu">brucejia@bu.edu</a>; <a href="mailto:shuyuej@ieee.org">shuyuej@ieee.org</a><br>
    
    <p><font color="#9933CC">𝗥𝗲𝘀𝗲𝗮𝗿𝗰𝗵 𝗧𝗲𝗮𝗺</font></p>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/team/Research_at_BU_2024_Spring.jpg" /><br><br>
      <p class="image-description">Our Research Team @ <a href="https://vkola-lab.github.io/team/">Kolachalama Laboratory</a></p>
      <p class="image-description">BU Center for Computing & Data Sciences, Boston, MA (Spring 2024)</p>
    </div>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/team/Research_at_BU_2024_Fall.jpg" /><br><br>
      <p class="image-description">Our Research Team @ <a href="https://vkola-lab.github.io/team/">Kolachalama Laboratory</a></p>
      <p class="image-description">BU Center for Computing & Data Sciences, Boston, MA (Fall 2024)</p>
    </div>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/team/Research_at_BU_2024_Winter_1.jpg" /><br><br>
      <p class="image-description">Our Research Team @ <a href="https://vkola-lab.github.io/team/">Kolachalama Laboratory</a></p>
      <p class="image-description">BU Center for Computing & Data Sciences, Boston, MA (Winter 2024)</p>
    </div>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/team/Research_at_BU_2024_Winter_2.jpg" /><br><br>
      <p class="image-description">Our Research Team @ <a href="https://vkola-lab.github.io/team/">Kolachalama Laboratory</a></p>
      <p class="image-description">BU Center for Computing & Data Sciences, Boston, MA (Winter 2024)</p>
    </div>
    
    <p><font color="#9933CC">𝗜𝗘𝗘𝗘 𝗧𝗲𝗮𝗺</font></p>
    <div class="profile-container">
      <img width="99%device-width" src="./imgs/IEEE/IEEE_2024.JPG" /><br><br>
      <p class="image-description">Our IEEE Local Conference Committee Team @ <a href="https://ieeeboston.org">IEEE Boston Section</a></p>
      <p class="image-description">Crowne Plaza, 15 Canal Park, Woburn, MA (Fall 2024)</p>
    </div>
    
  </details>
  
  <br>
  
  Email: <a href="mailto:brucejia@bu.edu">brucejia@bu.edu</a>; <a href="mailto:shuyuej@ieee.org">shuyuej@ieee.org</a>
  
  <br>
  <a href="./files/Resume/Curriculum_Vitae_Shuyue_JIA.pdf"><i class="fab fa fa-file-pdf-o"></i>&#32;Resume</a>
  <a href="https://scholar.google.com/citations?user=PfpEP60AAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-1x"></i>&#32;Scholar</a>
  <a href="https://huggingface.co/shuyuej">&#129303;&#32;HuggingFace</a>
  <a href="https://github.com/SuperBruceJia"><i class="fab fa-github"></i>&#32;GitHub</a>
  <iframe src="https://img.shields.io/github/followers/SuperBruceJia?label=follow&style=social" frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
  <iframe src="https://img.shields.io/github/stars/SuperBruceJia?label=stars&style=social" frameborder="0" scrolling="0" width="90px" height="20px"></iframe>
</p>

<h2 id="news">News</h2>
<ul style="list-style:none; margin:0; padding:0">
  <li>
    <strong>Aug 2025</strong> A <a href="./files/Presentation/Evidence-Retrieval-and-Grounding.pdf"> Presentation</a> of Evidence Retrieval and Grounding in Medicine.
  </li>
  <li>
    <strong>Aug 2025</strong> Our <a href="https://www.medrxiv.org/content/10.1101/2025.08.06.25333160v1"> Medical Evidence Retrieval & Grounding Agentic System </a> is available online!
  </li>
  <li>
    <strong>July 2025</strong> Open Source <a href="https://github.com/SuperBruceJia/Awesome-Text-Generation-Evaluation"> Awesome Text Generation Evaluation</a>, a curated list of evaluation metrics for Natural Language Generation (NLG)
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-Text-Generation-Evaluation?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-Text-Generation-Evaluation?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>May 2025</strong> Our paper <a href="https://www.nature.com/articles/s44385-025-00022-0">PodGPT</a> is accepted by <em>Nature npj Biomedical Innovations</em>.
    <br>𝗠𝗲𝗱𝗶𝗮 𝗖𝗼𝘃𝗲𝗿𝗮𝗴𝗲: 
    <a href="https://www.bu.edu/hic/2025/07/15/new-ai-model-podgpt-blends-research-and-podcasts-for-smarter-health-answers"> Boston University</a>; 
    <a href="https://www.bumc.bu.edu/camed/news-events/articles/2025/vijaya-b-kolachalama-phd-creates-new-language-model-to-help-people-obtain-more-accurate-answers-to-science-health-questions"> Boston University School of Medicine</a>; 
    <a href="https://phys.org/news/2025-07-podgpt-ai-science-podcasts.html"> Phys.org</a>.
  </li>
  <li>
    <strong>Mar 2025</strong> Honored to receive the <a href="https://issuu.com/ieeeboston/docs/ieee_boston_section_the_reflector_april_2025/6">2025 IEEE Boston Section Arthur Winston Student Award</a> from the <a href="https://ieeeboston.org/">IEEE Boston Section</a>, <em>"For outstanding contributions to the Boston Section Local Conference Committee"</em>.
  </li>
  <li>
    <strong>Dec 2024</strong> We have released the source code for Our <a href="https://github.com/vkola-lab/PodGPT/tree/main/rag_pipeline"> PodGPT with Retrieval-Augmented Generation (RAG)</a>. This powerful pipeline allows our model to ground responses with the latest scientific evidence from top-tier literature, such as <em>The New England Journal of Medicine</em> (NEJM)!
  </li>
  <li>
    <strong>Nov 2024</strong> Our <a href="https://www.nature.com/articles/s44385-025-00022-0"> PodGPT preprint</a> is available online! It is an audio-augmented Large Language Model (LLM) for STEMM research and education.
  </li>
  <li>
    <strong>Oct 2024</strong> We have launched <b>PodRAG</b> on our <a href="https://podgpt.org/">PodGPT</a> platform with the advanced 𝗥𝗲𝘁𝗿𝗶𝗲𝘃𝗮𝗹-𝗔𝘂𝗴𝗺𝗲𝗻𝘁𝗲𝗱 𝗚𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗼𝗻 (𝗥𝗔𝗚) techniques! It is designed to provide 𝘁𝗵𝗲 𝗺𝗼𝘀𝘁 𝗮𝗰𝗰𝘂𝗿𝗮𝘁𝗲 𝗮𝗻𝗱 𝘂𝗽-𝘁𝗼-𝗱𝗮𝘁𝗲 𝗶𝗻𝗳𝗼𝗿𝗺𝗮𝘁𝗶𝗼𝗻 for medical education and research! Please try it out if you are interested!
  </li>
  <li>
    <strong>Aug 2024</strong> Open Source and Keep Updating <a href="https://github.com/SuperBruceJia/Awesome-Large-Vision-Language-Model"> Awesome Large Vision-Language Model (LVLM/MM-LLM)</a>, a curated list of Large Vision-Language Model
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-Large-Vision-Language-Model?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-Large-Vision-Language-Model?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>, and <a href="https://github.com/SuperBruceJia/Awesome-Mixture-of-Experts"> Awesome Mixture of Experts (MoE)</a>, a curated list of Mixture of Experts (MoE) and Mixture of Multimodal Experts (MoME)
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-Mixture-of-Experts?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-Mixture-of-Experts?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
    Welcome to contribute and work together!
  </li>
  <li>
    <strong>Aug 2024</strong> Release and maintain a collection of <a href="https://huggingface.co/collections/shuyuej/quantization-669ea25d2ea444924e543da2"> &#129303;&#32; Quantized Large Language Models</a> for public usage, offering AI solutions with reduced computational requirements.
  </li>
  <li>
    <strong>July 2024</strong> Open Source <a href="https://github.com/vkola-lab/PodGPT"> PodGPT Library</a>, a library for benchmarking multilingual medical Large Language Models (Medical LLMs)
    <iframe
      src="https://img.shields.io/github/stars/vkola-lab/PodGPT" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/vkola-lab/PodGPT" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>July 2024</strong> Our <a href="https://www.medrxiv.org/content/10.1101/2024.07.11.24310304v1"> MedPodGPT preprint</a> is available online! It is an audio-augmented Large Language Model (LLM) for medical research and education.
  </li>
  <li>
    <strong>June 2024</strong> Our AI Platform, <a href="https://podgpt.org/"> PodGPT</a>, is now accessible to the general public. It is an online platform for deploying our latest multimodal foundation models for education and research.
  </li>
  <li>
    <strong>June 2024</strong> Our paper <a href="https://ieeexplore.ieee.org/document/10566053"> MedSyn</a> is accepted by <em>IEEE T-MI</em>.
    <br>𝗠𝗲𝗱𝗶𝗮 𝗖𝗼𝘃𝗲𝗿𝗮𝗴𝗲: 
    <a href="https://www.techtarget.com/healthtechanalytics/news/366603215/Anatomy-aware-GenAI-shows-promise-in-medical-imaging"> TechTarget</a>; 
    <a href="https://www.bu.edu/hic/2024/08/06/lung-disease-anatomy-aware-ai-genai-bu"> Boston University</a>; 
    <a href="https://www.dbmi.pitt.edu/240313-2"> University of Pittsburgh</a>.
  </li>
  <li>
    <strong>May 2024</strong> 𝗣𝗵.𝗗. 𝗖𝗮𝗻𝗱𝗶𝗱𝗮𝗰𝘆 𝗥𝗲𝗽𝗼𝗿𝘁: <a href="./files/PhD_Candidacy/PhD_Candidacy_Report_Shuyue_Jia_May_2024.pdf">Preference Alignment via Reinforcement Learning from Human Feedback</a> and 𝗣𝗿𝗲𝘀𝗲𝗻𝘁𝗮𝘁𝗶𝗼𝗻: <a href="./files/PhD_Candidacy/PhD_Candidacy_Presentation_Shuyue_Jia_May_2024.pdf">Slides</a>.
  </li>
  <li>
    <strong>Jan 2024</strong> 🔥 We are releasing <a href="https://huggingface.co/datasets/shuyuej/GSM8K-Consistency">&#129303;&#32;GSM8K-Consistency</a>, a benchmark database for analyzing the consistency of Arithmetic Reasoning on GSM8K.
  </li>
  <li>
    <strong>Dec 2023</strong> Open Source <a href="https://github.com/SuperBruceJia/promptcraft"> 🔨 PromptCraft</a> and its published <a href="https://pypi.org/project/promptcraft"> PyPI Package</a>, a prompt perturbation toolkit from the character, word, and sentence levels for prompt robustness analysis
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/promptcraft?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/promptcraft?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Oct 2023</strong> Open Source <a href="https://github.com/SuperBruceJia/Awesome-Semantic-Textual-Similarity"> Awesome Semantic Textual Similarity</a>, a curated list of Semantic/Sentence Textual Similarity (STS) in Large Language Models (LLMs) and Natural Language Processing (NLP)
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-Semantic-Textual-Similarity?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-Semantic-Textual-Similarity?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Oct 2023</strong> A <a href="./files/Presentation/Sentence_Textual_Similarity-Model_Evolution_Overview.pdf"> Presentation</a> of Sentence Textual Similarity: Model Evolution Overview.
  </li>
  <li>
    <strong>Oct 2023</strong> Open Source <a href="https://github.com/SuperBruceJia/Awesome-LLM-Self-Consistency"> Awesome LLM Self-Consistency</a>, a curated paper and presentation list of self-consistency in Large Language Models (LLMs)
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/Awesome-LLM-Self-Consistency?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/Awesome-LLM-Self-Consistency?style=social" 
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Oct 2023</strong> A <a href="./files/Presentation/Prompt-Perturbation-and-Robustness-Evaluation-2023-Shuyue-JIA.pdf"> Presentation</a> of Prompt Perturbation and Robustness Evaluation.
  </li>
  <li>
    <strong>Sept 2023</strong> A <a href="./files/Presentation/Prompt-based-Learning-and-Robustness-Evaluation-2023-Shuyue-JIA.pdf"> Presentation</a> of Prompt-based Learning and Robustness Evaluation.
  </li>
  <li>
    <strong>Sept 2023</strong> A <a href="./files/Presentation/Self-Consistency-Benefits-Large-Language-Models-2023-Shuyue-JIA.pdf"> Presentation</a> of Self-Consistency Benefits Large Language Models.
  </li>
  <li>
    <strong>Sept 2023</strong> Our paper <a href="https://ieeexplore.ieee.org/abstract/document/10251073/"> Deep Transfer Learning</a> is accepted by <em>IEEE T-IM</em>.
  </li>
  <li>
    <strong>May 2023</strong> 𝗠.𝗣𝗵𝗶𝗹.𝗧𝗵𝗲𝘀𝗶𝘀: <a href="https://scholars.cityu.edu.hk/en/theses/noreference-image-quality-assessment-via-nonlocal-modeling(2d1e72fb-2405-43df-aac9-4838b6da1875).html"> No-reference Image Quality Assessment via Non-local Modeling</a> and 𝗗𝗲𝗳𝗲𝗻𝘀𝗲 𝗦𝗹𝗶𝗱𝗲𝘀: <a href="./files/Thesis/MPhil-Thesis-Defense-Presentation-ShuyueJia.pdf"> Image Quality Assessment and Perceptual Optimization: A Non-local Modeling Approach</a>.
  </li>
  <li>
    <strong>Mar 2023</strong> A <a href="./files/Presentation/Foundation-Models-Sequential-Decision-Making.pdf"> Presentation</a> of Foundation Models for Sequential Decision Making.
  </li>
  <li>
    <strong>Jan 2023</strong> A <a href="./files/Presentation/A_Summary_Three_Projects.pdf"> Presentation</a> of IQA Regression and EEG Classification.
  </li>
  <li>
    <strong>Nov 2022</strong> A <a href="./files/Proposal/Research_Proposal_VPS.pdf"> Research Proposal</a> of Video Panoptic Segmentation (VPS).
  </li>
  <li>
    <strong>Aug 2022</strong> Our paper <a href="https://ieeexplore.ieee.org/document/9889159"> GCNs-Net</a> is accepted by <em>IEEE T-NNLS</em>
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/EEG-DL?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/EEG-DL?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Aug 2022</strong> Our paper <a href="https://ieeexplore.ieee.org/abstract/document/9950035"> NLNet</a> is accepted by <em>IEEE MMSP</em>. Source codes are available on <a href="https://github.com/SuperBruceJia/NLNet-IQA">&#32;GitHub</a>
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/NLNet-IQA?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/NLNet-IQA?style=social"
      frameborder="0" scrolling="0" width="82px" height="20px">
    </iframe>, and the trained models are available on <a href="https://huggingface.co/shuyuej/NLNet">&#129303;&#32;HuggingFace</a> for real-life IQA inference.
  </li>
  <li>
    <strong>Dec 2021</strong> Our paper <a href="https://www.frontiersin.org/article/10.3389/fbioe.2021.706229"> BiLSTM-GCNs</a> is accepted by <em>Frontiers in Bioengineering and Biotechnology</em>.
  </li>
  <li>
    <strong>Apr 2020</strong> Open Source <a href="https://github.com/SuperBruceJia/EEG-DL"> EEG-DL</a>, a Deep Learning (DL) library written by TensorFlow for EEG Signals Classification
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/EEG-DL?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/EEG-DL?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>.
  </li>
  <li>
    <strong>Feb 2020</strong> Our paper<a href="https://iopscience.iop.org/article/10.1088/1741-2552/ab4af6/meta"> ESI-CNNs</a> is accepted by <em>Journal of Neural Engineering</em> 
    <iframe
      src="https://img.shields.io/github/stars/SuperBruceJia/EEG-Motor-Imagery-Classification-CNNs-TensorFlow?style=social"
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>
    <iframe
      src="https://img.shields.io/github/forks/SuperBruceJia/EEG-Motor-Imagery-Classification-CNNs-TensorFlow?style=social" 
      frameborder="0" scrolling="0" width="90px" height="20px">
    </iframe>.
  </li>
</ul>

<h2 id="publications">Publications</h2>
<h3 id="Generative AI"><font color="black">Topic 1 - </font><font color="#DE3163">Generative AI and Foundation Models</font></h3>
<ol>
  <li>
    <div style="float:left;">
      <font color="#00066f">Agentic Memory-augmented Retrieval and Evidence Grounding in Medicine</font>
    </div>
    <div style="float:right;">
      <a href="./files/Presentation/Evidence-Retrieval-and-Grounding.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://www.medrxiv.org/content/10.1101/2025.08.06.25333160v1"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    <strong>Shuyue Jia</strong>, Subhrangshu Bit, Varuna H. Jasodanand, Yi Liu, Vijaya B. Kolachalama
    <br><em>Technical Report</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/Agent/Medical-Agent.jpg" alt="Medical AI Agent Project">
          </div>
        </div>
        We developed a unified, open-source LLM-based agentic system that integrates document retrieval, re-ranking, evidence grounding, and diagnosis generation to support dynamic, multi-step medical reasoning. Our system features a lightweight retrieval-augmented generation pipeline coupled with a cache-and-prune memory bank, enabling efficient long-context inference beyond standard LLM limits. The system autonomously invokes specialized tools, eliminating the need for manual prompt engineering or brittle multi-stage templates.
      </ul>
    </details>
  </li>

  <hr>

  <li>
    <div style="float:left;">
      <font color="#00066f">PodGPT: An Audio-augmented Large Language Model for Research and Education</font>
    </div>
    <div style="float:right;">
      <a href="https://medpodgpt.org"><i class="fa fa-codepen"></i> Product</a>
      <a href="./files/Presentation/Evidence-Retrieval-and-Grounding.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/vkola-lab/podgpt"><i class="fa fa-github"></i> Codes</a>
      <a href="https://www.nature.com/articles/s44385-025-00022-0"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    <strong>Shuyue Jia</strong>, Subhrangshu Bit, Edward Searls, Meagan V. Lauber, Lindsey A. Claus, Pengrui Fan, Varuna H. Jasodanand, Divya Veerapaneni, William M. Wang, Rhoda Au, Vijaya B. Kolachalama
    <br><em>Nature npj Biomedical Innovations</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/PodGPT/PodGPT.png" alt="PodGPT Project">
          </div>
        </div>
        Here, we introduce PodGPT, an audio-augmented large language model (LLM) tailored for research and education. The process began by leveraging publicly available generative AI auto-regressive language models across various scales. These models underwent continuous pre-training on a curated dataset of English CC-BY podcasts produced by scientific journals and clinical experts, as well as content from <em>The New England Journal of Medicine</em> (NEJM). The podcast corpus comprised over 3,700 hours of audio, covering diverse topics in science, research, and medicine, visually summarized in the accompanying word cloud. The next phase involved developing the software infrastructure, which included an inference engine for model deployment, a messaging queue, database integration, retrieval augmented generation (RAG) implementation, API microservices, and a responsive human-machine interface. This highly performant and robust system enabled users with internet access to engage seamlessly with current research and educational material via an adaptive chatbot. The chatbot supported multi-turn conversations across various languages, empowering users to access and interact with STEMM knowledge in a dynamic and accessible manner.
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">MedPodGPT: A Multilingual Audio-augmented Large Language Model for Medical Research and Education</font>
    </div>
    <div style="float:right;">
      <a href="https://medpodgpt.org"><i class="fa fa-codepen"></i> Product</a>
      <a href="./files/Presentation/Evidence-Retrieval-and-Grounding.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/vkola-lab/medpodgpt"><i class="fa fa-github"></i> Codes</a>
      <a href="https://www.medrxiv.org/content/10.1101/2024.07.11.24310304v1"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    <strong>Shuyue Jia</strong>, Subhrangshu Bit, Edward Searls, Lindsey A. Claus, Pengrui Fan, Varuna H. Jasodanand, Meagan V. Lauber, Divya Veerapaneni, William M. Wang, Rhoda Au, Vijaya B. Kolachalama
    <br><em>Technical Report</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/PodGPT/MedPodGPT.png" alt="MedPodGPT Project">
          </div>
        </div>
        Here, we introduce MedPodGPT, an audio-augmented large language model (LLM) designed for medical research and education. Medical podcasts offer audio content rich in specialized terminology, diverse medical topics, and expert dialogues, helping the medical community stay current with the latest information. Integrating this content into LLMs can enhance their ability to provide up-to-date clinical information.
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images</font>
    </div>
    <div style="float:right;">
      <a href="https://github.com/batmanlab/MedSyn"><i class="fa fa-github"></i> Codes</a>
      <a href="https://ieeexplore.ieee.org/document/10566053"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    Yanwu Xu, Li Sun, Wei Peng, <strong>Shuyue Jia</strong>, Katelyn Morrison, Adam Perer, Afrooz Zandifar, Shyam Visweswaran, Motahhare Eslami, Kayhan Batmanghelich
    <br><em>IEEE Transactions on Medical Imaging</em> (IEEE T-MI)
  </li>
</ol>
  
<hr>

<h3 id="Computer Vision"><font color="black">Topic 2 - </font><font color="#DE3163">Computer Vision</font></h3>
<ol>
  <li>
    <div style="float:left;">
      <font color="#00066f">No-reference Image Quality Assessment via Non-local Dependency Modeling</font>
    </div><br>
    <strong>Shuyue Jia</strong>, Dingquan Li, Shiqi Wang
    <div style="float:right;">
      <a href="./files/MMSP/MMSP22_Poster.pdf"><i class="fa fa-file-powerpoint-o"></i> Poster</a>
      <a href="./files/Presentation/A_Summary_Three_Projects.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/SuperBruceJia/NLNet-IQA"><i class="fa fa-github"></i> Codes</a>
      <a href="https://huggingface.co/shuyuej/NLNet">&#129303;&#32;HuggingFace</a>
      <a href="https://ieeexplore.ieee.org/abstract/document/9950035"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div>
    <br><em>IEEE 24th International Workshop on Multimedia Signal Processing</em> (IEEE MMSP)
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        A no-reference image quality assessment method based on non-local features learned by a graph neural network (GNN). The proposed quality assessment framework is rooted in the view that the human visual system perceives image quality with long-dependency constructed among different regions, inspiring us to explore the non-local interactions in quality prediction.
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Research/Picture5.jpg" alt="NL-Net">
        </div>
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">Learning From Mixed Datasets: A Monotonic Image Quality Assessment Model</font>
    </div>
    <div style="float:right;">
      <a href="https://github.com/SuperBruceJia/MonotonicIQA"><i class="fa fa-github"></i> Codes</a>
      <a href="https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/ell2.12698"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    Zhaopeng Feng, Keyang Zhang, <strong>Shuyue Jia</strong>, Baoliang Chen, Shiqi Wang
    <br><em>IET Electronics Letters</em>
  </li>

</ol>

<hr>

<h3 id="Neuroscience"><font color="black">Topic 3 - </font><font color="#DE3163">Neuroscience</font></h3>
<ol>
  <li>
    <div style="float:left;">
      <font color="#00066f">
        GCNs-Net: A Graph Convolutional Neural Network Approach for Decoding Time-resolved EEG Motor Imagery Signals
      </font>
    </div>
    <strong>Shuyue Jia</strong>, Yimin Hou, Xiangmin Lun, Yan Shi, Yang Li, Rui Zeng, Jinglei Lv
    <br>
    
    <div style="float:left;">
      <em>IEEE Transactions on Neural Networks and Learning Systems</em> (IEEE T-NNLS)
    </div>
    <div style="float:right;">
      <a href="./files/EEG/GCNs-Net-Presentation.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/SuperBruceJia/EEG-DL"><i class="fa fa-github"></i> Codes</a>
      <a href="https://ieeexplore.ieee.org/document/9889159"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        Traditional works classify EEG signals without considering the topological relationship among electrodes. Thus, a graph convolutional neural network is presented while cooperating with the functional topological relationship of electrodes.
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="imgs/Research/Picture2.png" alt="Project2">
          </div>
        </div>
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor Imagery Recognition</font>
    </div><br>
    Yimin Hou, <strong>Shuyue Jia (Corresponding Author)</strong>, Xiangmin Lun, Shu Zhang, Jinglei Lv
    <div style="float:right;">
      <a href="./files/EEG/BiLSTM-GCN-Presentation.pdf"><i class="fa fa-file-powerpoint-o"></i> Slides</a>
      <a href="https://github.com/SuperBruceJia/EEG-DL"><i class="fa fa-github"></i> Codes</a>
      <a href="https://www.frontiersin.org/article/10.3389/fbioe.2021.706229"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div>
    <br><em>Frontiers in Bioengineering and Biotechnology</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        This paper presents a novel deep learning approach designed toward both remarkably accurate and responsive motor imagery (MI) recognition based on scalp EEG. Bidirectional long short-term memory (BiLSTM) with the attention mechanism is employed, and the graph convolutional neural network (GCN) promotes the decoding performance by cooperating with the topological structure of features.
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="./imgs/Research/Picture3.jpeg" alt="Project3.1">
            <img width="99%device-width" src="./imgs/Research/Picture4.jpeg" alt="Project3.2">
          </div>
        </div>
      </ul>
    </details>
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f"> 
        A Novel Approach of Decoding EEG Four-Class Motor Imagery Tasks via Scout ESI and CNN
      </font>
    </div>
    <div style="float:right;">
      <a href="https://github.com/SuperBruceJia/EEG-Motor-Imagery-Classification-CNNs-TensorFlow"><i class="fab fa fa-github"></i> Codes</a>
      <a href="https://iopscience.iop.org/article/10.1088/1741-2552/ab4af6/meta"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div><br>
    Yimin Hou, Lu Zhou, <strong>Shuyue Jia</strong>, Xiangmin Lun
    <br><em>Journal of Neural Engineering</em>
    <details>
      <summary style="outline:none">See More</summary>
      <ul>
        We presented a novel approach that could potentially improve the current stroke rehabilitation strategies by implementing a deep learning approach for an Electroencephalogram (EEG) based on MI Brain-Computer Interface System.
        <ul>
          <li>
            Constructed 6 convolutional layers, 2 max-pooling layers, and 3 FC layers CNNs for four-class motor imagery classification through TensorFlow, with 50% dropout (spatial dropout after every Conv layer and regular dropout for FC layers) &ndash; 11.44% accuracy improvement, batch normalization (BN) &ndash; 10.15% improvement, and Short-cut Connection &ndash; 1.76% improvement to prevent overfitting, and achieved SOTA results: 94.50% accuracy on scout R5, 94.54% at subject level, and 96% for left fist prediction.
          </li>
          <li>
            Took charge of DNNs design, including methods comparisons, such as MLPs, CNNs, RNNs, and LSTMs, classification results calculations, and programming. 10 and 14 subjects&rsquo; data were utilized (19,320 and 27,048 samples in the experiments)
          </li>
          <li>
            Benchmark Dataset: <a href="https://archive.physionet.org/pn4/eegmmidb/"> EEG Motor Movement/Imagery Dataset</a>.
          </li>
        </ul>
        <div>
          <div style="text-align:center">
            <img width="99%device-width" src="./imgs/Research/Picture1-1.png" alt="EEG-CNN-1">
            <img width="50%device-width" src="./imgs/Research/Picture1-2.png" alt="EEG-CNN-2">
          </div>
        </div>
      </ul>
    </details>
  </li>
</ol>

<hr>

<h3 id="Intelligent Technologies"><font color="black">Topic 4 - </font><font color="#DE3163">Intelligent Technologies</font></h3>
<ol>
  <li>
    <div style="float:left;">
      <font color="#00066f">PMU Measurements based Short-term Voltage Stability Assessment of Power Systems via Deep Transfer Learning</font>
    </div><br>
    Yang Li, Shitu Zhang, Yuanzheng Li, Jiting Cao, <strong>Shuyue Jia</strong>
    <div style="float:right;">
      <a href="https://ieeexplore.ieee.org/abstract/document/10251073"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div>
    <br><em>IEEE Transactions on Instrumentation and Measurement</em> (IEEE T-IM)
  </li>
  
  <hr>
  
  <li>
    <div style="float:left;">
      <font color="#00066f">Improving Performance: A Collaborative Strategy for the Multi-data Fusion of Electronic Nose and Hyperspectral to Track the Quality Difference of Rice</font>
    </div><br>
    Yan Shi, Hangcheng Yuan, Chenao Xiong, Qi Zhang, <strong>Shuyue Jia</strong>, Jingjing Liu, Hong Men
    <div style="float:right;">
      <a href="https://www.sciencedirect.com/science/article/pii/S0925400521001143"><i class="fab fa fa-file-pdf-o"></i> Paper</a>
    </div>
    <br><em>Sensors and Actuators B: Chemical</em>
  </li>
</ol>

<h2 id="Teaching Experience">Teaching Experience</h2>
<ol>
  <li><strong>Teaching Assistant</strong>, Spring 2025 Semester<br>
    <a href="https://www.bu.edu/academics/eng/courses/eng-ec-327/" style="color:black;">ENG EC 327: Introduction to Software Engineering</a>, Boston University<br>
  </li>
</ol>

<h2 id="Academic Services">Academic Services</h2>
<ol>
  <li><strong>Journal Reviewer</strong> of<br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046" style="color:black;">IEEE Transactions on Multimedia (IEEE T-MM)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" style="color:black;">IEEE Transactions on Circuits and Systems for Video Technology (IEEE T-CSVT)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" style="color:black;">IEEE Transactions on Neural Networks and Learning Systems (IEEE T-NNLS)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42" style="color:black;">IEEE Transactions on Medical Imaging (IEEE T-MI)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10" style="color:black;">IEEE Transactions on Biomedical Engineering (IEEE T-BME)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9424" style="color:black;">IEEE Transactions on Industrial Informatics (IEEE T-II)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221020" style="color:black;">IEEE Journal of Biomedical and Health Informatics (IEEE JBHI)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4200690" style="color:black;">IEEE Journal of Selected Topics in Signal Processing (IEEE J-STSP)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8782706" style="color:black;">IEEE Open Journal of the Industrial Electronics Society (IEEE OJIES)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8782664" style="color:black;">IEEE Open Journal of the Computer Society (IEEE OJCS)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=93" style="color:black;">IEEE MultiMedia (MM)</a><br>
    <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7361" style="color:black;">IEEE Sensors Journal</a><br>
    <a href="https://www.jmir.org" style="color:black;">Journal of Medical Internet Research</a>
  </li>
  <li><strong>Conference Reviewer</strong> of<br>
    <a href="https://iclr.cc" style="color:black;">The International Conference on Learning Representations (ICLR) 2025</a>
  </li>
  <li><strong>Committee Member</strong> of<br>
    <ul>
      <li><a href="https://ieeeboston.org" style="color:black;">Local Conference Committee, IEEE Boston Section</a></li>
      <li><a href="https://ieee-icad.org/?page_id=121" style="color:black;">Speaker Program Chair (Keynotes & Panel Sessions), <br>
        2025 IEEE International Conference on AI and Data Analytics (ICAD)</a></li>
      <li><a href="https://amia.org/education-events/amia-2025-clinical-informatics-conference/spc" style="color:black;">Scientific Program Committee Member, 2025 AMIA Clinical Informatics Conference</a></li>
    </ul>
  </li>
  <li><strong>Member</strong> of <a href="./awards/SigmaXi_2025.pdf" style="color:black;">Sigma Xi</a>, <a href="https://www.ieee.org" style="color:black;">IEEE</a>, <a href="https://www.acm.org" style="color:black;">ACM</a>, <a href="https://www.aclweb.org/portal/what-is-cl" style="color:black;">ACL</a>, and <a href="https://aaai.org" style="color:black;">AAAI</a></li>
</ol>

<h2 id="Selected Awards">Selected Awards</h2>
<ol>
  <li>
    <div style="float:left;">
      <font color="black">
        2025 IEEE Boston Section Arthur Winston Student Award, IEEE Boston Section
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none"><em>"For outstanding contributions to the Boston Section Local Conference Committee"</em></summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./awards/IEEE-Boston-Section-Award.jpg" alt="IEEE-2025">
        </div>
        <div style="text-align:center">
          <img width="99%device-width" src="./awards/IEEE_Award_2025_1.jpg" alt="IEEE-2025">
        </div>
        <div style="text-align:center">
          <img width="99%device-width" src="./awards/IEEE_Award_2025_2.jpg" alt="IEEE-2025">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        Wiley Top Cited Article 2023-2024, Wiley Top Cited Papers, Wiley
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none"><em>"Our work has been recognized as a top cited article in Electronics Letters"</em></summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./awards/Wiley-Top-Cited-Article-Certificate-2023-2024.jpg" alt="IEEE-2025">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        CityU Top 5 Runner, City University of Hong Kong
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">Athlete</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Marathon/cityu_marathon_2021.jpg" alt="marathon-2021">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        Outstanding Athlete, Northeast Electric Power University
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">Athlete</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Marathon/EliteAthlete.jpg" alt="Elite Athlete">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        3000-meter Steeplechase, The 45th Northeast Electric Power University Games
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">The 7th Place in college</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Marathon/Steeplechase.jpg" alt="Steeplechase">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2015 National High School Math League, China
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">Second Prize</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Competition/Math.jpg" alt="Math">
        </div>
      </div>
    </details>
  </li>
  
  <li>
    <div style="float:left;">
      <font color="black">
        The 32nd Chinese Physics Olympiad (CPhO), China
      </font>
    </div>
    
    <br>
    
    <details>
      <summary style="outline:none">Third Prize</summary>
      <div>
        <div style="text-align:center">
          <img width="99%device-width" src="./imgs/Competition/Physics.jpg" alt="Physics">
        </div>
      </div>
    </details>
  </li>  
</ol>

<h2 id="Academic Services">International Marathon Athlete Activities</h2>
<ol>
  <li>
    <div style="float:left;">
      <font color="black">
        2025 The 129th Boston Marathon, Abbott World Marathon Majors
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2024 Boston Half Marathon
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2023 Standard Chartered Hong Kong Marathon, Full Marathon
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2021 Hangzhou International Marathon, Half Marathon
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2021 Standard Chartered Hong Kong Marathon, Half Marathon
      </font>
    </div>
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2018 National Marathon Championships (Jilin City Station), Full Marathon
      </font>
    </div>
    
  </li>
  
  <br>
  
  <li>
    <div style="float:left;">
      <font color="black">
        2017 National Marathon Championships (Jilin City Station), Half Marathon
      </font>
    </div>
    
  </li>
</ol>

<br>
<br>

</div>
</div>

<hr>
<br>
</body>

<footer style="text-align:center">
  <a style="font-family: 'STKaiti'; font-size: 20px; display: block; margin-bottom: 2px; color: black;text-decoration: none;">文章千古事，得失寸心知。</a>
  𝙁𝙞𝙣𝙙 𝙩𝙝𝙚 𝙘𝙤𝙪𝙧𝙖𝙜𝙚 𝙩𝙤 𝙗𝙚𝙮𝙤𝙣𝙙 𝙗𝙤𝙪𝙣𝙙𝙖𝙧𝙞𝙚𝙨!<br>
  <br>
  <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">
    <img alt="Creative Commons License" style="border-width:0"
      src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" />
  </a>
  <span><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></span>
</footer>

<br>
<br>
<br>
<br>
</html>